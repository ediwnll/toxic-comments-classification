{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from textblob import TextBlob\n",
    "from nltk.tokenize import WordPunctTokenizer, word_tokenize, StanfordSegmenter, sent_tokenize, PunktSentenceTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import re, os, sys, string, itertools\n",
    "from collections import defaultdict, Counter\n",
    "import unicodedata\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "import sentencepiece as spm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to replace words which are written in quotes with quoted word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_quotes(series):\n",
    "    \"\"\"\n",
    "    Function removes quotes from words or phrases written in double quotes.\n",
    "    \"\"\"\n",
    "    # Define a regex pattern to match words or phrases in quotes\n",
    "    regex = re.compile(r'\"([\\w\\s]+)\"')\n",
    "    \n",
    "    # Count the number of occurrences of quoted words/phrases\n",
    "    total_occurrences = series.str.count(regex).sum()\n",
    "    print(f\"Total occurrences of quoted words: {total_occurrences}\")\n",
    "    \n",
    "    # Remove the quotes but keep the words inside\n",
    "    series = series.str.replace(regex, r'\\1', regex=True)\n",
    "    \n",
    "    return series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removal of IP Address found in text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_ips(series):\n",
    "    \"\"\"\n",
    "    Removing Ip Addresses\n",
    "    \"\"\"\n",
    "    series = series.copy()\n",
    "    regex = re.compile(r'(([0-9]{1,}\\.){2,}[0-9]{1,})')\n",
    "    print(\"Total unique ip address in data are {}\".format(series.str.extract(regex).nunique()))\n",
    "    series = series.str.replace(regex, ' ', regex = True)\n",
    "    return series\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_trailing_dates(series):\n",
    "    series = series.copy()\n",
    "    return series.str.replace(\"([0-9]{1,2}:[0-9]{1,2},{0,1}\\s[0-9]{1,2}\\s[a-zA-Z]{3,}\\s[0-9]{2,4}\\s\\((utc|UTC)\\))\", \" \", regex = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_repetitions(series, thresh=5):\n",
    "    \"\"\"\n",
    "    If comment has repetitions, if more than repetition then trim at 10 words\n",
    "    \"\"\"\n",
    "    series = series.copy()\n",
    "    total_words = series.str.count(\"\\w+\")\n",
    "    unique_words = series.apply(lambda x:len(np.unique(x.split(' '))))\n",
    "\n",
    "    rep_inds = total_words/unique_words > thresh\n",
    "    print(\"Total comments with high repetitions are {}\".format(sum(rep_inds)))\n",
    "    print(\"Some examples of high reps are {}\".format(series.loc[rep_inds].sample(5).values))\n",
    "\n",
    "    rep_inds_unq = iter(unique_words.loc[rep_inds])\n",
    "    series.loc[rep_inds] = series[rep_inds].str.split(' ').str.slice(0, next(rep_inds_unq)).str.join(' ')\n",
    "\n",
    "    print(\"Some samples are {}\".format(series.loc[rep_inds].sample(5).values))\n",
    "    return series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def break_oovocabwords(series, vocab_filename, sp_file):\n",
    "    \"\"\"\n",
    "    Break OOV words using SentencePiece and preprocess the text.\n",
    "    \"\"\"\n",
    "    series = series.copy()\n",
    "\n",
    "    # Load the vocabulary\n",
    "    with open(vocab_filename, encoding='utf-8') as f:\n",
    "        dict_word = set([o.rstrip().rsplit(' ')[0] for o in f])\n",
    "\n",
    "    # Load SentencePiece model\n",
    "    sp = spm.SentencePieceProcessor()\n",
    "    sp.Load(sp_file)\n",
    "\n",
    "    def standardize_repeated_chars(word):\n",
    "        \"\"\"\n",
    "        Replace 3+ repeated characters with 2 (e.g., 'wayyyyy' -> 'wayy').\n",
    "        \"\"\"\n",
    "        return re.sub(r'(.)\\1{2,}', r'\\1\\1', word)\n",
    "\n",
    "    # Preprocess each word in the series\n",
    "    series = series.apply(lambda x: ' '.join(itertools.chain.from_iterable(\n",
    "        [sp.EncodeAsPieces(standardize_repeated_chars(word)) if word not in dict_word else [word] for word in x.split(' ')]\n",
    "    )))\n",
    "\n",
    "    # Remove SentencePiece artifacts\n",
    "    series = series.str.replace(\"â–\", \"\", regex=False)\n",
    "\n",
    "    return series\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_url(series):\n",
    "    series = series.copy()\n",
    "    regex = re.compile(r'http[s]?://\\S+')\n",
    "    series = series.str.replace(regex, ' ', regex = True)\n",
    "    return series\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(series, remove_ip=True, remove_date_stamps=True, tag_quoted=True, remove_puncts=True, lower=True,\n",
    "                    remove_digits=True, remove_nonchars=True,\n",
    "                   break_oov=True, break_vocab_file=\"\", break_sp_file=\"\", trim_reps=True):\n",
    "    series = series.copy()\n",
    "    series = series.str.replace(r\"\\\\n{1,}\", \" line \")\n",
    "\n",
    "    if remove_url:\n",
    "        series = remove_url(series)\n",
    "    \n",
    "    if remove_ip:\n",
    "        series = remove_ips(series)\n",
    "        \n",
    "    if remove_date_stamps:\n",
    "        series = remove_trailing_dates(series)\n",
    "        \n",
    "    if tag_quoted:\n",
    "        series = remove_quotes(series)\n",
    "        \n",
    "    if remove_puncts:\n",
    "        series = series.str.replace(\"'\", \"\")\n",
    "        series = series.str.translate(str.maketrans({s:\" \" for s in string.punctuation}))\n",
    "        \n",
    "    if lower:\n",
    "        series = series.str.lower()\n",
    "        \n",
    "    if remove_digits:\n",
    "        series = series.str.replace(r\"\\d\", \"\", regex = True )\n",
    "    \n",
    "    if remove_nonchars:\n",
    "        series = series.str.replace(r\"[^a-zA-Z0-9.,\\\"!]+\", \" \", regex = True)\n",
    "        \n",
    "    if break_oov:\n",
    "        series = break_oovocabwords(series, break_vocab_file, break_sp_file)\n",
    "        \n",
    "    if trim_reps:\n",
    "        series = trim_repetitions(series, thresh=10)\n",
    "        \n",
    "    return series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "train.head()\n",
    "\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object, got 'Series'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[57], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m embed_file \u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mglove.42B.300d.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      2\u001b[0m sp_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men.wiki.bpe.vs200000.model\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 4\u001b[0m train\u001b[38;5;241m.\u001b[39mcomment_text \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcomment_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbreak_vocab_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membed_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbreak_sp_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msp_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m test\u001b[38;5;241m.\u001b[39mcomment_text \u001b[38;5;241m=\u001b[39m preprocess_text(test\u001b[38;5;241m.\u001b[39mcomment_text, break_vocab_file\u001b[38;5;241m=\u001b[39membed_file, break_sp_file\u001b[38;5;241m=\u001b[39msp_file)\n\u001b[0;32m      6\u001b[0m train\u001b[38;5;241m.\u001b[39mcomment_text\u001b[38;5;241m.\u001b[39msample(\u001b[38;5;241m10\u001b[39m)\u001b[38;5;241m.\u001b[39mvalues\n",
      "Cell \u001b[1;32mIn[55], line 8\u001b[0m, in \u001b[0;36mpreprocess_text\u001b[1;34m(series, remove_ip, remove_date_stamps, tag_quoted, remove_puncts, lower, remove_digits, remove_nonchars, break_oov, break_vocab_file, break_sp_file, trim_reps)\u001b[0m\n\u001b[0;32m      5\u001b[0m series \u001b[38;5;241m=\u001b[39m series\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m1,}\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m line \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remove_url:\n\u001b[1;32m----> 8\u001b[0m     series \u001b[38;5;241m=\u001b[39m \u001b[43mremove_url\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseries\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remove_ip:\n\u001b[0;32m     11\u001b[0m     series \u001b[38;5;241m=\u001b[39m remove_ips(series)\n",
      "Cell \u001b[1;32mIn[54], line 3\u001b[0m, in \u001b[0;36mremove_url\u001b[1;34m(series)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mremove_url\u001b[39m(series):\n\u001b[0;32m      2\u001b[0m     series \u001b[38;5;241m=\u001b[39m series\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m----> 3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mre\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msub\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhttp[s]?://\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mS+\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseries\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\hewzh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\re\\__init__.py:185\u001b[0m, in \u001b[0;36msub\u001b[1;34m(pattern, repl, string, count, flags)\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msub\u001b[39m(pattern, repl, string, count\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, flags\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m    179\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the string obtained by replacing the leftmost\u001b[39;00m\n\u001b[0;32m    180\u001b[0m \u001b[38;5;124;03m    non-overlapping occurrences of the pattern in string by the\u001b[39;00m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;124;03m    replacement repl.  repl can be either a string or a callable;\u001b[39;00m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;124;03m    if a string, backslash escapes in it are processed.  If it is\u001b[39;00m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;124;03m    a callable, it's passed the Match object and must return\u001b[39;00m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;124;03m    a replacement string to be used.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msub\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstring\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcount\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: expected string or bytes-like object, got 'Series'"
     ]
    }
   ],
   "source": [
    "embed_file =\"glove.42B.300d.txt\"\n",
    "sp_file = \"en.wiki.bpe.vs200000.model\"\n",
    "\n",
    "train.comment_text = preprocess_text(train.comment_text, break_vocab_file=embed_file, break_sp_file=sp_file)\n",
    "test.comment_text = preprocess_text(test.comment_text, break_vocab_file=embed_file, break_sp_file=sp_file)\n",
    "train.comment_text.sample(10).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_encoding_artifacts(series):\n",
    "    \"\"\"\n",
    "    Removes multiple encoding artifacts from the text.\n",
    "    \"\"\"\n",
    "    # Add patterns for all known artifacts\n",
    "    pattern = r'â–|â–|â€œ|â€|â€¦'\n",
    "    \n",
    "    # Replace all matches with an empty string\n",
    "    series = series.str.replace(pattern, '', regex=True)\n",
    "    return series\n",
    "\n",
    "train.comment_text = remove_encoding_artifacts(train.comment_text)\n",
    "test.comment_text = remove_encoding_artifacts(test.comment_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv(\"train_preprocess_v3.csv\", index=False)\n",
    "test.to_csv(\"test_preprocess_v3.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
