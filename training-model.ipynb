{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 id                                       comment_text  toxic  \\\n",
      "0  0000997932d777bf  explanation why the edits made under my userna...      0   \n",
      "1  000103f0d9cfb60f  daww he matches this background colour im seem...      0   \n",
      "2  000113f07ec002fd  hey man im really not trying to edit war its j...      0   \n",
      "3  0001b41b1c6bb37e  more i cant make any real suggestions on impro...      0   \n",
      "4  0001d958c54c6e35  you sir are my hero any chance you remember wh...      0   \n",
      "\n",
      "   severe_toxic  obscene  threat  insult  identity_hate  \n",
      "0             0        0       0       0              0  \n",
      "1             0        0       0       0              0  \n",
      "2             0        0       0       0              0  \n",
      "3             0        0       0       0              0  \n",
      "4             0        0       0       0              0  \n",
      "                 id                                       comment_text\n",
      "0  00001cee341fdb12  yo bitch ja rule is more succesful then youll ...\n",
      "1  0000247867823ef7            from rfc the title is fine as it is imo\n",
      "2  00013b17ad220c46                     sources zawe ashton on lapland\n",
      "3  00017563c3f7919a  if you have a look back at the source the info...\n",
      "4  00017695ad8997eb            i dont anonymously edit articles at all\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_train = pd.read_csv('train_preprocess_v3.csv')\n",
    "df_test = pd.read_csv('test_preprocess_v3.csv')\n",
    "\n",
    "print(df_train.head())\n",
    "print(df_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id                 0\n",
      "comment_text     102\n",
      "toxic              0\n",
      "severe_toxic       0\n",
      "obscene            0\n",
      "threat             0\n",
      "insult             0\n",
      "identity_hate      0\n",
      "dtype: int64\n",
      "id                0\n",
      "comment_text    906\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Checking for null values\n",
    "print(df_train.isnull().sum())\n",
    "print(df_test.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.dropna(subset=['comment_text'])\n",
    "df_test = df_test.dropna(subset=['comment_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAApAAAAHHCAYAAAAI1miCAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAULtJREFUeJzt3XlcVdX+//H3EWTmIBqiOIDzLI6Zs90sNLNssszMIaerZloOdSvnucmhwcquQ5p1K7M0Nc0ccsgBxRHJVNRSwzRA1BRh/f7wx/56BJWNKASv5+NxHg/O2mvv/VlbPLwfaw/HYYwxAgAAADKpQE4XAAAAgH8WAiQAAABsIUACAADAFgIkAAAAbCFAAgAAwBYCJAAAAGwhQAIAAMAWAiQAAABsIUACAADAFgIkgJvSpUsXhYWFZWndESNGyOFwZG9ByJS0Y//nn39m2zZv5nfhRsLCwtSlS5dbsu0rxcbGyuFwaNasWVZbly5d5Ofnd8v3ncbhcGjEiBG3bX9AVhAggTzK4XBk6rV69eqcLjVHdOnSxeU4+Pn5qWzZsnrsscf01VdfKTU1Ncvb/vTTTzV58uTsK/b/a9GihapXr57t273dWrRoYR33AgUKyOl0qlKlSurUqZNWrFiRbftZsmRJrg1iubk2IDPcc7oAALfGJ5984vJ+zpw5WrFiRbr2KlWq3NR+PvrooyyHrVdffVUvvfTSTe3/Znh6emrGjBmSpPPnz+vw4cNatGiRHnvsMbVo0ULffPONnE6n7e1++umn2r17twYMGJDNFecdJUuW1Pjx4yVJZ8+e1a+//qoFCxZo7ty5at++vebOnauCBQta/WNiYlSggL05jyVLlujdd9+1FdRCQ0N1/vx5l33fCter7fz583J3588zcjd+Q4E86umnn3Z5//PPP2vFihXp2q927tw5+fj4ZHo/N/OH1t3dPUf/ULq7u6c7HmPGjNGECRP08ssvq0ePHvr8889zqLq8LSAgIN2xnzBhgvr376/33ntPYWFhmjhxorXM09PzltZz6dIlpaamysPDQ15eXrd0XzeS0/sHMoNT2EA+lnZKNDIyUs2aNZOPj4/+85//SJK++eYbtWnTRiEhIfL09FS5cuU0evRopaSkuGzj6uve0q4he+ONN/Thhx+qXLly8vT0VP369bVlyxaXdTO6BtLhcKhfv35auHChqlevLk9PT1WrVk3Lli1LV//q1atVr149eXl5qVy5cvrggw+y5brKl156Sffdd5+++OIL/fLLL1Z7Zo5JixYt9N133+nw4cPWadq043Px4kUNGzZMdevWVUBAgHx9fdW0aVOtWrXqpuq90s6dO9WlSxeVLVtWXl5eKlasmLp166ZTp05l2P/PP/9U+/bt5XQ6VaRIET3//PP6+++/0/WbO3eu6tatK29vbxUuXFhPPvmkjh49mm11S5Kbm5umTp2qqlWr6p133lFCQoK17OprIJOTkzVy5EhVqFBBXl5eKlKkiJo0aWKdAu/SpYveffddSa6Xc0iuv6OTJ0+2fkf37t2b4TWQaQ4ePKiIiAj5+voqJCREo0aNkjHGWr569eoMLwu5epvXqy2t7eqZye3bt6t169ZyOp3y8/PTPffco59//tmlz6xZs+RwOLR+/Xq98MILCgoKkq+vrx5++GGdPHnyxv8AgA3MQAL53KlTp9S6dWs9+eSTevrppxUcHCzp8h8jPz8/vfDCC/Lz89OPP/6oYcOGKTExUa+//voNt/vpp5/qzJkz6tWrlxwOhyZNmqRHHnlEBw8evOGs5bp167RgwQL16dNH/v7+mjp1qh599FEdOXJERYoUkXT5D2qrVq1UvHhxjRw5UikpKRo1apSCgoJu/qBI6tSpk5YvX64VK1aoYsWKkjJ3TF555RUlJCTot99+09tvvy1J1g0YiYmJmjFjhjp06KAePXrozJkz+vjjjxUREaHNmzerVq1aN133ihUrdPDgQXXt2lXFihXTnj179OGHH2rPnj36+eef04Xr9u3bKywsTOPHj9fPP/+sqVOn6q+//tKcOXOsPmPHjtVrr72m9u3bq3v37jp58qSmTZumZs2aafv27SpUqNBN153Gzc1NHTp00GuvvaZ169apTZs2GfYbMWKExo8fr+7du+vOO+9UYmKitm7dqm3btunee+9Vr169dOzYsQwv20gzc+ZM/f333+rZs6c8PT1VuHDha16OkZKSolatWumuu+7SpEmTtGzZMg0fPlyXLl3SqFGjbI0xM7Vdac+ePWratKmcTqeGDBmiggUL6oMPPlCLFi20Zs0aNWjQwKX/c889p8DAQA0fPlyxsbGaPHmy+vXrx2w6spcBkC/07dvXXP1fvnnz5kaSmT59err+586dS9fWq1cv4+PjY/7++2+rrXPnziY0NNR6f+jQISPJFClSxJw+fdpq/+abb4wks2jRIqtt+PDh6WqSZDw8PMyvv/5qte3YscNIMtOmTbPa2rZta3x8fMzvv/9ute3fv9+4u7un22ZGOnfubHx9fa+5fPv27UaSGThwoNWW2WPSpk0bl2OS5tKlS+bChQsubX/99ZcJDg423bp1u2HNzZs3N9WqVbtun4xqnD9/vpFk1q5da7WlHfsHH3zQpW+fPn2MJLNjxw5jjDGxsbHGzc3NjB071qXfrl27jLu7u0v71b8LWR3H119/bSSZKVOmWG2hoaGmc+fO1vvw8HDTpk2b6+4no995Y/7vd9TpdJq4uLgMl82cOdNq69y5s5FknnvuOastNTXVtGnTxnh4eJiTJ08aY4xZtWqVkWRWrVp1w21eqzZjLv8fGD58uPW+Xbt2xsPDwxw4cMBqO3bsmPH39zfNmjWz2mbOnGkkmZYtW5rU1FSrfeDAgcbNzc3Ex8dnuD8gKziFDeRznp6e6tq1a7p2b29v6+czZ87ozz//VNOmTXXu3Dnt27fvhtt94oknFBgYaL1v2rSppMunAW+kZcuWKleunPW+Zs2acjqd1ropKSn64Ycf1K5dO4WEhFj9ypcvr9atW99w+5mRNmt45swZq+1mj4mbm5s8PDwkSampqTp9+rQuXbqkevXqadu2bdlS95U1/v333/rzzz911113SVKG++jbt6/L++eee07S5Zs8JGnBggVKTU1V+/bt9eeff1qvYsWKqUKFCtl6+j1NRsf+aoUKFdKePXu0f//+LO/n0UcftTVj3a9fP+vntEstLl68qB9++CHLNdxISkqKli9frnbt2qls2bJWe/HixfXUU09p3bp1SkxMdFmnZ8+eLjPNTZs2VUpKig4fPnzL6kT+Q4AE8rkSJUpYoeZKe/bs0cMPP6yAgAA5nU4FBQVZNz1ceW3atZQuXdrlfVqY/Ouvv2yvm7Z+2rpxcXE6f/68ypcvn65fRm1ZkZSUJEny9/e32m72mEjS7NmzVbNmTeu6vaCgIH333XeZXv9GTp8+reeff17BwcHy9vZWUFCQypQpc80aK1So4PK+XLlyKlCggGJjYyVJ+/fvlzFGFSpUUFBQkMsrOjpacXFx2VL3lTI69lcbNWqU4uPjVbFiRdWoUUODBw/Wzp07be0n7bhkRoECBVwCnCTr0oa0Y3UrnDx5UufOnVOlSpXSLatSpYpSU1PTXYt6M//3gMziGkggn7tyxipNfHy8mjdvLqfTqVGjRqlcuXLy8vLStm3bNHTo0Ew9tsfNzS3DdnPFTQe3Yt3ssnv3bkn/F0iz45jMnTtXXbp0Ubt27TR48GAVLVpUbm5uGj9+vA4cOJAtdbdv314bNmzQ4MGDVatWLfn5+Sk1NVWtWrXKVI1XXyOZmpoqh8OhpUuXZvjvcisesH31sc9Is2bNdODAAX3zzTdavny5ZsyYobffflvTp09X9+7dM7WfjH73b8a1bt66+sazWy03/P9B3keABJDO6tWrderUKS1YsEDNmjWz2g8dOpSDVf2fokWLysvLS7/++mu6ZRm1ZcUnn3wih8Ohe++9V5K9Y3KtIPHll1+qbNmyWrBggUuf4cOHZ0vNf/31l1auXKmRI0dq2LBhVvv1TvPu37/fZSbu119/VWpqqnXneLly5WSMUZkyZawZt1spJSVFn376qXx8fNSkSZPr9i1cuLC6du2qrl27KikpSc2aNdOIESOsAJmd33KUmpqqgwcPuhyDtDv0045V2kxffHy8y7oZnTrObG1BQUHy8fFRTExMumX79u1TgQIFVKpUqUxtC8hOnMIGkE7aDMaVMxYXL17Ue++9l1MluXBzc1PLli21cOFCHTt2zGr/9ddftXTp0pve/oQJE7R8+XI98cQT1ileO8fE19c3w9PFGW1j06ZN2rhx403XfK3tS7rut+KkPU4mzbRp0yTJupb0kUcekZubm0aOHJluu8aYaz4eKCtSUlLUv39/RUdHq3///td9iPvV+/Xz81P58uV14cIFq83X11dS+kCXVe+88471szFG77zzjgoWLKh77rlH0uWHkLu5uWnt2rUu613rdyQztbm5uem+++7TN99843Kq/I8//tCnn36qJk2aZOlh98DNYgYSQDqNGjVSYGCgOnfurP79+8vhcOiTTz7JVafARowYoeXLl6tx48b697//rZSUFL3zzjuqXr26oqKiMrWNS5cuae7cuZIu33By+PBhffvtt9q5c6fuvvtuffjhh1ZfO8ekbt26+vzzz/XCCy+ofv368vPzU9u2bfXAAw9owYIFevjhh9WmTRsdOnRI06dPV9WqVa3r/m7k5MmTGjNmTLr2MmXKqGPHjmrWrJkmTZqk5ORklShRQsuXL7/uzPGhQ4f04IMPqlWrVtq4caPmzp2rp556SuHh4ZIuz0COGTNGL7/8smJjY9WuXTv5+/vr0KFD+vrrr9WzZ08NGjQoU7VfKSEhwTr2586ds76J5sCBA3ryySc1evTo665ftWpVtWjRQnXr1lXhwoW1detWffnlly43utStW1eS1L9/f0VERMjNzU1PPvmk7Vqlyw/3XrZsmTp37qwGDRpo6dKl+u677/Sf//zHuhEnICBAjz/+uKZNmyaHw6Fy5cpp8eLFGV4naqe2MWPGaMWKFWrSpIn69Okjd3d3ffDBB7pw4YImTZqUpfEANy0nbv0GcPtd6zE+13qcyvr1681dd91lvL29TUhIiBkyZIj5/vvv0z2m5FqP8Xn99dfTbVNXPZ7kWo/x6du3b7p1r36MizHGrFy50tSuXdt4eHiYcuXKmRkzZpgXX3zReHl5XeMo/J+0R7OkvXx8fExYWJh59NFHzZdffmlSUlKyfEySkpLMU089ZQoVKmQkWccnNTXVjBs3zoSGhhpPT09Tu3Zts3jxYluPv7my5itf99xzjzHGmN9++808/PDDplChQiYgIMA8/vjj5tixY9c89nv37jWPPfaY8ff3N4GBgaZfv37m/Pnz6fb91VdfmSZNmhhfX1/j6+trKleubPr27WtiYmJcjmlWxuHn52cqVKhgnn76abN8+fIM17n633/MmDHmzjvvNIUKFTLe3t6mcuXKZuzYsebixYtWn0uXLpnnnnvOBAUFGYfDYf2uXe939FqP8fH19TUHDhww9913n/Hx8THBwcFm+PDh6X5PTp48aR599FHj4+NjAgMDTa9evczu3bvTbfNatRmT/v+JMcZs27bNREREGD8/P+Pj42Puvvtus2HDBpc+aY/x2bJli0v7tR4vBNwMhzG5aEoBAG5Su3btbvrxLgCA6+MaSAD/WOfPn3d5v3//fi1ZskQtWrTImYIAIJ9gBhLAP1bx4sWt730+fPiw3n//fV24cEHbt29P93xDAED24SYaAP9YrVq10vz583XixAl5enqqYcOGGjduHOERAG4xZiABAABgC9dAAgAAwBYCJAAAAGzhGkhIuvw1XceOHZO/v3+2fv0XAAC4dYwxOnPmjEJCQlSgwO2bFyRAQpJ07Ngxvk8VAIB/qKNHj6pkyZK3bX8ESEiS/P39JV3+BeR7VQEA+GdITExUqVKlrL/jtwsBEpJknbZ2Op0ESAAA/mFu9+Vn3EQDAAAAWwiQAAAAsIUACQAAAFu4BhIumr06X26e3jldBgAAeUbk68/kdAnZjhlIAAAA2EKABAAAgC0ESAAAANhCgAQAAIAtBEgAAADYQoAEAACALQRIAAAA2EKABAAAgC0ESAAAANhCgAQAAIAtBEgAAADYQoAEAACALQRIAAAA2EKABAAAgC0ESAAAANhCgAQAAIAtBEgAAADYQoAEAACALQRIAAAA2EKA/IcKCwvT5MmTc7oMAACQD7nndAH5QYsWLVSrVq1sDXxbtmyRr69vtm0PAAAgswiQ/1BBQUE5XQIAAMinOIV9i3Xp0kVr1qzRlClT5HA45HA4FBsbqzVr1ujOO++Up6enihcvrpdeekmXLl2SJM2ZM0d+fn7av3+/tZ0+ffqocuXKOnfunKT0p7Dj4+PVq1cvBQcHy8vLS9WrV9fixYtv61gBAED+wAzkLTZlyhT98ssvql69ukaNGiVJSklJ0f33368uXbpozpw52rdvn3r06CEvLy+NGDFCzzzzjBYvXqyOHTtqw4YN+v777zVjxgxt3LhRPj4+6faRmpqq1q1b68yZM5o7d67KlSunvXv3ys3N7Zp1XbhwQRcuXLDeJyYmZv/gAQBAnkSAvMUCAgLk4eEhHx8fFStWTJL0yiuvqFSpUnrnnXfkcDhUuXJlHTt2TEOHDtWwYcNUoEABffDBB6pZs6b69++vBQsWaMSIEapbt26G+/jhhx+0efNmRUdHq2LFipKksmXLXreu8ePHa+TIkdk7WAAAkC9wCjsHREdHq2HDhnI4HFZb48aNlZSUpN9++02SFBgYqI8//ljvv/++ypUrp5deeuma24uKilLJkiWt8JgZL7/8shISEqzX0aNHsz4gAACQrzADmYutXbtWbm5uOn78uM6ePSt/f/8M+3l7e9vetqenpzw9PW+2RAAAkA8xA3kbeHh4KCUlxXpfpUoVbdy4UcYYq239+vXy9/dXyZIlJUkbNmzQxIkTtWjRIvn5+alfv37X3H7NmjX122+/6Zdffrl1gwAAAPj/CJC3QVhYmDZt2qTY2Fj9+eef6tOnj44eParnnntO+/bt0zfffKPhw4frhRdeUIECBXTmzBl16tRJ/fv3V+vWrTVv3jx9/vnn+vLLLzPcfvPmzdWsWTM9+uijWrFihQ4dOqSlS5dq2bJlt3mkAAAgPyBA3gaDBg2Sm5ubqlatqqCgICUnJ2vJkiXavHmzwsPD1bt3bz377LN69dVXJUnPP/+8fH19NW7cOElSjRo1NG7cOPXq1Uu///57hvv46quvVL9+fXXo0EFVq1bVkCFDXGY9AQAAsovDXHkeFflWYmKiAgICFP7cdLl52r+mEgAAZCzy9Wdu2bbT/n4nJCTI6XTesv1cjRlIAAAA2EKABAAAgC0ESAAAANhCgAQAAIAtBEgAAADYQoAEAACALQRIAAAA2EKABAAAgC0ESAAAANhCgAQAAIAtBEgAAADYQoAEAACALQRIAAAA2EKABAAAgC0ESAAAANhCgAQAAIAtBEgAAADYQoAEAACALe45XQByl7VjOsjpdOZ0GQAAIBdjBhIAAAC2ECABAABgCwESAAAAthAgAQAAYAsBEgAAALYQIAEAAGALARIAAAC2ECABAABgCwESAAAAthAgAQAAYAsBEgAAALbwXdhw0ezV+XLz9M7pMgDkApGvP5PTJQDIpZiBBAAAgC0ESAAAANhCgAQAAIAtBEgAAADYQoAEAACALQRIAAAA2EKABAAAgC0ESAAAANhCgAQAAIAtBEgAAADYQoAEAACALQRIAAAA2EKABAAAgC0ESAAAANhCgAQAAIAtBEgAAADYQoAEAACALQRIAAAA2EKAvIHVq1fL4XAoPj4+p0sBAADIFQiQAAAAsIUACQAAAFsIkJIuXLig/v37q2jRovLy8lKTJk20ZcsWlz7r169XzZo15eXlpbvuuku7d++2lh0+fFht27ZVYGCgfH19Va1aNS1ZssRavmfPHj3wwANyOp3y9/dX06ZNdeDAAWv5jBkzVKVKFXl5ealy5cp67733rGWxsbFyOBxasGCB7r77bvn4+Cg8PFwbN250qW/dunVq2rSpvL29VapUKfXv319nz57N7kMFAABAgJSkIUOG6KuvvtLs2bO1bds2lS9fXhERETp9+rTVZ/DgwXrzzTe1ZcsWBQUFqW3btkpOTpYk9e3bVxcuXNDatWu1a9cuTZw4UX5+fpKk33//Xc2aNZOnp6d+/PFHRUZGqlu3brp06ZIkad68eRo2bJjGjh2r6OhojRs3Tq+99ppmz57tUuMrr7yiQYMGKSoqShUrVlSHDh2sbRw4cECtWrXSo48+qp07d+rzzz/XunXr1K9fv9tx+AAAQD7jMMaYnC4iJ509e1aBgYGaNWuWnnrqKUlScnKywsLCNGDAANWvX1933323PvvsMz3xxBOSpNOnT6tkyZKaNWuW2rdvr5o1a+rRRx/V8OHD023/P//5jz777DPFxMSoYMGC6ZaXL19eo0ePVocOHay2MWPGaMmSJdqwYYNiY2NVpkwZzZgxQ88++6wkae/evapWrZqio6NVuXJlde/eXW5ubvrggw+sbaxbt07NmzfX2bNn5eXllW6/Fy5c0IULF6z3iYmJKlWqlMKfmy43T+8sHk0AeUnk68/kdAkAbiAxMVEBAQFKSEiQ0+m8bfvN9zOQBw4cUHJysho3bmy1FSxYUHfeeaeio6OttoYNG1o/Fy5cWJUqVbKW9+/fX2PGjFHjxo01fPhw7dy50+obFRWlpk2bZhgez549qwMHDujZZ5+Vn5+f9RozZozLKW5JqlmzpvVz8eLFJUlxcXGSpB07dmjWrFku24iIiFBqaqoOHTqU4bjHjx+vgIAA61WqVKlMHzMAAJC/5fsAmR26d++ugwcPqlOnTtq1a5fq1aunadOmSZK8va89m5eUlCRJ+uijjxQVFWW9du/erZ9//tml75UB1OFwSJJSU1Ot7fTq1ctlGzt27ND+/ftVrly5DPf98ssvKyEhwXodPXo06wcAAADkK/k+QJYrV04eHh5av3691ZacnKwtW7aoatWqVtuVge6vv/7SL7/8oipVqlhtpUqVUu/evbVgwQK9+OKL+uijjyRdnjn86aefrOslrxQcHKyQkBAdPHhQ5cuXd3mVKVMm02OoU6eO9u7dm24b5cuXl4eHR4breHp6yul0urwAAAAyI98HSF9fX/373//W4MGDtWzZMu3du1c9evTQuXPnrGsOJWnUqFFauXKldu/erS5duuiOO+5Qu3btJEkDBgzQ999/r0OHDmnbtm1atWqVFS779eunxMREPfnkk9q6dav279+vTz75RDExMZKkkSNHavz48Zo6dap++eUX7dq1SzNnztRbb72V6TEMHTpUGzZsUL9+/RQVFaX9+/frm2++4SYaAABwS7jndAG5wYQJE5SamqpOnTrpzJkzqlevnr7//nsFBga69Hn++ee1f/9+1apVS4sWLbJm91JSUtS3b1/99ttvcjqdatWqld5++21JUpEiRfTjjz9q8ODBat68udzc3FSrVi3rmsvu3bvLx8dHr7/+ugYPHixfX1/VqFFDAwYMyHT9NWvW1Jo1a/TKK6+oadOmMsaoXLly1k0/AAAA2Snf34WNy9Lu4uIubABpuAsbyP24CxsAAAD/CARIAAAA2EKABAAAgC0ESAAAANhCgAQAAIAtBEgAAADYQoAEAACALQRIAAAA2EKABAAAgC0ESAAAANhCgAQAAIAtBEgAAADYQoAEAACALQRIAAAA2EKABAAAgC0ESAAAANhCgAQAAIAtBEgAAADY4p7TBSB3WTumg5xOZ06XAQAAcjFmIAEAAGALARIAAAC2ECABAABgCwESAAAAthAgAQAAYAsBEgAAALYQIAEAAGALARIAAAC2ECABAABgCwESAAAAthAgAQAAYAvfhQ0XzV6dLzdP75wuA8BNiHz9mZwuAUAexwwkAAAAbCFAAgAAwBYCJAAAAGwhQAIAAMAWAiQAAABsIUACAADAFgIkAAAAbCFAAgAAwBYCJAAAAGwhQAIAAMAWAiQAAABsIUACAADAFgIkAAAAbCFAAgAAwBYCJAAAAGwhQAIAAMAWAiQAAABsIUACAADAFgIkAAAAbCFAZrMWLVpowIABt2Vfq1evlsPhUHx8/G3ZHwAAgCS553QBec2CBQtUsGDBHNn3rFmzNGDAAAIlAAC4pQiQ2axw4cI5XQIAAMAtxSnsbHblKeywsDCNGzdO3bp1k7+/v0qXLq0PP/zQ6nvx4kX169dPxYsXl5eXl0JDQzV+/HhJUmxsrBwOh6Kioqz+8fHxcjgcWr16dbr9rl69Wl27dlVCQoIcDoccDodGjBhxC0cKAADyKwLkLfbmm2+qXr162r59u/r06aN///vfiomJkSRNnTpV3377rf73v/8pJiZG8+bNU1hYWJb206hRI02ePFlOp1PHjx/X8ePHNWjQoGv2v3DhghITE11eAAAAmcEp7Fvs/vvvV58+fSRJQ4cO1dtvv61Vq1apUqVKOnLkiCpUqKAmTZrI4XAoNDQ0y/vx8PBQQECAHA6HihUrdsP+48eP18iRI7O8PwAAkH8xA3mL1axZ0/o5LdzFxcVJkrp06aKoqChVqlRJ/fv31/Lly29bXS+//LISEhKs19GjR2/bvgEAwD8bAfIWu/qObIfDodTUVElSnTp1dOjQIY0ePVrnz59X+/bt9dhjj0mSChS4/E9jjLHWTU5Ozra6PD095XQ6XV4AAACZQYDMYU6nU0888YQ++ugjff755/rqq690+vRpBQUFSZKOHz9u9b3yhpqMeHh4KCUl5VaWCwAAwDWQOemtt95S8eLFVbt2bRUoUEBffPGFihUrpkKFCqlAgQK66667NGHCBJUpU0ZxcXF69dVXr7u9sLAwJSUlaeXKlQoPD5ePj498fHxu02gAAEB+wQxkDvL399ekSZNUr1491a9fX7GxsVqyZIl1+vq///2vLl26pLp162rAgAEaM2bMdbfXqFEj9e7dW0888YSCgoI0adKk2zEMAACQzzjMlRfZId9KTExUQECAwp+bLjdP75wuB8BNiHz9mZwuAcBtkvb3OyEh4bbez8AMJAAAAGwhQAIAAMAWAiQAAABsIUACAADAFgIkAAAAbMm2ABkfH59dmwIAAEAulqUAOXHiRH3++efW+/bt26tIkSIqUaKEduzYkW3FAQAAIPfJUoCcPn26SpUqJUlasWKFVqxYoaVLl6p169YaPHhwthYIAACA3CVLX2V44sQJK0AuXrxY7du313333aewsDA1aNAgWwsEAABA7pKlGcjAwEAdPXpUkrRs2TK1bNlSkmSMUUpKSvZVBwAAgFwnSzOQjzzyiJ566ilVqFBBp06dUuvWrSVJ27dvV/ny5bO1QAAAAOQuWQqQb7/9tsLCwnT06FFNmjRJfn5+kqTjx4+rT58+2VogAAAAcpcsBciCBQtq0KBB6doHDhx40wUBAAAgd8t0gPz2228zvdEHH3wwS8UAAAAg98t0gGzXrl2m+jkcDm6kAQAAyMMyHSBTU1NvZR0AAAD4h7jprzL8+++/s6MOAAAA/ENkKUCmpKRo9OjRKlGihPz8/HTw4EFJ0muvvaaPP/44WwsEAABA7pKlu7DHjh2r2bNna9KkSerRo4fVXr16dU2ePFnPPvtsthWI22vtmA5yOp05XQYAAMjFsjQDOWfOHH344Yfq2LGj3NzcrPbw8HDt27cv24oDAABA7pOlAPn7779n+I0zqampSk5OvumiAAAAkHtlKUBWrVpVP/30U7r2L7/8UrVr177pogAAAJB7ZekayGHDhqlz5876/ffflZqaqgULFigmJkZz5szR4sWLs7tGAAAA5CJZmoF86KGHtGjRIv3www/y9fXVsGHDFB0drUWLFunee+/N7hoBAACQiziMMSani0DOS0xMVEBAgBISErgLGwCAf4ic+vudpVPYabZu3aro6GhJl6+LrFu3brYUBQAAgNwrSwHyt99+U4cOHbR+/XoVKlRIkhQfH69GjRrps88+U8mSJbOzRgAAAOQiWboGsnv37kpOTlZ0dLROnz6t06dPKzo6WqmpqerevXt21wgAAIBcJEvXQHp7e2vDhg3pHtkTGRmppk2b6ty5c9lWIG4ProEEAOCfJ6f+fmdpBrJUqVIZPjA8JSVFISEhN10UAAAAcq8sXQP5+uuv67nnntO7776revXqSbp8Q83zzz+vN954I1sLxO3V7NX5cvP0zukysizy9WdyugQAAPK8TAfIwMBAORwO6/3Zs2fVoEEDubtf3sSlS5fk7u6ubt26qV27dtleKAAAAHKHTAfIyZMn38IyAAAA8E+R6QDZuXPnW1kHAAAA/iFu6kHikvT333/r4sWLLm3cxQsAAJB3Zeku7LNnz6pfv34qWrSofH19FRgY6PICAABA3pWlADlkyBD9+OOPev/99+Xp6akZM2Zo5MiRCgkJ0Zw5c7K7RgAAAOQiWTqFvWjRIs2ZM0ctWrRQ165d1bRpU5UvX16hoaGaN2+eOnbsmN11AgAAIJfI0gzk6dOnVbZsWUmXr3c8ffq0JKlJkyZau3Zt9lUHAACAXCdLAbJs2bI6dOiQJKly5cr63//+J+nyzGRAQED2VQcAAIBcJ0sBsmvXrtqxY4ck6aWXXtK7774rLy8vDRw4UEOGDMnWAgEAAJC7ZOkayIEDB1o/t2zZUvv27VNkZKTuuOMOzZ07N9uKAwAAQO6TpRnIq4WGhuqRRx5RQECAPv744+zYJAAAAHKpbAmQAAAAyD8IkAAAALCFAAkAAABbbN1E88gjj1x3eXx8/M3UAgAAgH8AWwHyRs94DAgI0DPPPHNTBQEAACB3sxUgZ86ceavqAAAAwD8E10ACAADAFgLkP9SIESNUq1atnC4DAADkQwTIW2zWrFkqVKhQtm930KBBWrlyZbZvFwAA4Eay9FWG+UVKSoocDocKFMh9OdvPz09+fn45XQYAAMiHcl0y+vLLL1WjRg15e3urSJEiatmypc6ePStJmjFjhqpUqSIvLy9VrlxZ7733nrVeo0aNNHToUJdtnTx5UgULFtTatWslSRcuXNCgQYNUokQJ+fr6qkGDBlq9erXVP2228Ntvv1XVqlXl6empI0eO3HC9a1m9erW6du2qhIQEORwOORwOjRgxQpL0119/6ZlnnlFgYKB8fHzUunVr7d+/36q7WLFiGjdunLWtDRs2yMPDw5p1zOgU9n//+19Vq1ZNnp6eKl68uPr165epYw4AAGBHrgqQx48fV4cOHdStWzdFR0dr9erVeuSRR2SM0bx58zRs2DCNHTtW0dHRGjdunF577TXNnj1bktSxY0d99tlnMsZY2/v8888VEhKipk2bSpL69eunjRs36rPPPtPOnTv1+OOPq1WrVlZwk6Rz585p4sSJmjFjhvbs2aOiRYtmar2MNGrUSJMnT5bT6dTx48d1/PhxDRo0SJLUpUsXbd26Vd9++602btwoY4zuv/9+JScnKygoSP/97381YsQIbd26VWfOnFGnTp3Ur18/3XPPPRnu6/3331ffvn3Vs2dP7dq1S99++63Kly9/zdouXLigxMRElxcAAEBmOMyViSuHbdu2TXXr1lVsbKxCQ0NdlpUvX16jR49Whw4drLYxY8ZoyZIl2rBhg06ePKmQkBD9+OOPVmBs1KiRmjVrpgkTJujIkSMqW7asjhw5opCQEGsbLVu21J133qlx48Zp1qxZ6tq1q6KiohQeHi5JmVrvembNmqUBAwa4PGR9//79qlixotavX69GjRpJkk6dOqVSpUpp9uzZevzxxyVJffv21Q8//KB69epp165d2rJlizw9PSVdnoFcuHChoqKiJEklSpRQ165dNWbMmEwd6xEjRmjkyJHp2sOfmy43T+9MbSM3inyd55ACAPKPxMREBQQEKCEhQU6n87btN1ddAxkeHq577rlHNWrUUEREhO677z499thj8vDw0IEDB/Tss8+qR48eVv9Lly5ZDzcPCgrSfffdp3nz5qlp06Y6dOiQNm7cqA8++ECStGvXLqWkpKhixYou+7xw4YKKFClivffw8FDNmjWt95ldz47o6Gi5u7urQYMGVluRIkVUqVIlRUdHW21vvPGGqlevri+++EKRkZFWeLxaXFycjh07ds3ZyYy8/PLLeuGFF6z3iYmJKlWqVBZGAwAA8ptcFSDd3Ny0YsUKbdiwQcuXL9e0adP0yiuvaNGiRZKkjz76yCV0pa2TpmPHjurfv7+mTZumTz/9VDVq1FCNGjUkSUlJSXJzc1NkZKTLOpJcbkbx9vaWw+Gw3md2vVvhwIEDOnbsmFJTUxUbG2uN5Wre3vZnDD09Pa8ZSAEAAK4nVwVISXI4HGrcuLEaN26sYcOGKTQ0VOvXr1dISIgOHjyojh07XnPdhx56SD179tSyZcv06aefunytYu3atZWSkqK4uDjrFHdmZHW9NB4eHkpJSXFpq1Klii5duqRNmza5nMKOiYlR1apVJUkXL17U008/rSeeeEKVKlVS9+7dtWvXLhUtWjTdPvz9/RUWFqaVK1fq7rvvtl0jAACAHbkqQG7atEkrV67Ufffdp6JFi2rTpk06efKkqlSpopEjR6p///4KCAhQq1atdOHCBW3dulV//fWXdSrW19dX7dq102uvvabo6GiX6yUrVqyojh076plnntGbb76p2rVr6+TJk1q5cqVq1qypNm3aZFhTVtdLExYWpqSkJK1cuVLh4eHy8fFRhQoV9NBDD6lHjx764IMP5O/vr5deekklSpTQQw89JEl65ZVXlJCQoKlTp8rPz09LlixRt27dtHjx4gz3M2LECPXu3VtFixZV69atdebMGa1fv17PPfdcVv4pAAAArilX3YXtdDq1du1a3X///apYsaJeffVVvfnmm2rdurW6d++uGTNmaObMmapRo4aaN2+uWbNmqUyZMi7b6Nixo3bs2KGmTZuqdOnSLstmzpypZ555Ri+++KIqVaqkdu3aacuWLen6XS2r60mXb+Tp3bu3nnjiCQUFBWnSpEnWNuvWrasHHnhADRs2lDFGS5YsUcGCBbV69WpNnjxZn3zyiZxOpwoUKKBPPvlEP/30k95///0M99O5c2dNnjxZ7733nqpVq6YHHnjghneJAwAAZEWuugsbOSftLi7uwgYA4J8jp+7CzlUzkAAAAMj9CJA3qXXr1tbXCl79utEzIgEAAP6JctVNNP9EM2bM0Pnz5zNcVrhw4dtcDQAAwK1HgLxJJUqUyOkSAAAAbitOYQMAAMAWAiQAAABsIUACAADAFgIkAAAAbCFAAgAAwBYCJAAAAGwhQAIAAMAWAiQAAABsIUACAADAFgIkAAAAbCFAAgAAwBYCJAAAAGwhQAIAAMAW95wuALnL2jEd5HQ6c7oMAACQizEDCQAAAFsIkAAAALCFAAkAAABbCJAAAACwhQAJAAAAWwiQAAAAsIUACQAAAFsIkAAAALCFAAkAAABbCJAAAACwhQAJAAAAWwiQAAAAsMU9pwtA7tLs1fly8/TO6TJuKPL1Z3K6BAAA8i1mIAEAAGALARIAAAC2ECABAABgCwESAAAAthAgAQAAYAsBEgAAALYQIAEAAGALARIAAAC2ECABAABgCwESAAAAthAgAQAAYAsBEgAAALYQIAEAAGALARIAAAC2ECABAABgCwESAAAAthAgAQAAYAsBEgAAALbkaIBs0aKFBgwYcM3lYWFhmjx58i2vY/Xq1XI4HIqPj79l++jSpYvatWt3y7YPAABwu+RogFywYIFGjx59W/eZUWht1KiRjh8/roCAAEnSrFmzVKhQodtaV2bcjqALAABwI+45ufPChQvn5O4tHh4eKlasWE6XAQAA8I+Qa05hx8XFqW3btvL29laZMmU0b968dP3j4+PVvXt3BQUFyel06l//+pd27NhhLR8xYoRq1aqlTz75RGFhYQoICNCTTz6pM2fOSLp8GnnNmjWaMmWKHA6HHA6HYmNjXWb2Vq9era5duyohIcHqM2LECI0aNUrVq1dPV1OtWrX02muvZXrMb7zxhooXL64iRYqob9++Sk5OtpZ98sknqlevnvz9/VWsWDE99dRTiouLkyTFxsbq7rvvliQFBgbK4XCoS5cukqTU1FSNHz9eZcqUkbe3t8LDw/Xll19muiYAAAA7cs1NNF26dNHRo0e1atUqffnll3rvvfes8JTm8ccfV1xcnJYuXarIyEjVqVNH99xzj06fPm31OXDggBYuXKjFixdr8eLFWrNmjSZMmCBJmjJliho2bKgePXro+PHjOn78uEqVKuWyj0aNGmny5MlyOp1Wn0GDBqlbt26Kjo7Wli1brL7bt2/Xzp071bVr10yNcdWqVTpw4IBWrVql2bNna9asWZo1a5a1PDk5WaNHj9aOHTu0cOFCxcbGWiGxVKlS+uqrryRJMTExOn78uKZMmSJJGj9+vObMmaPp06drz549GjhwoJ5++mmtWbPmmrVcuHBBiYmJLi8AAIDMyNFT2Gl++eUXLV26VJs3b1b9+vUlSR9//LGqVKli9Vm3bp02b96suLg4eXp6Sro8m7dw4UJ9+eWX6tmzp6TLs3GzZs2Sv7+/JKlTp05auXKlxo4dq4CAAHl4eMjHx+eap6w9PDwUEBAgh8Ph0sfPz08RERGaOXOmVePMmTPVvHlzlS1bNlPjDAwM1DvvvCM3NzdVrlxZbdq00cqVK9WjRw9JUrdu3ay+ZcuW1dSpU1W/fn0lJSXJz8/POuVftGhR6xrNCxcuaNy4cfrhhx/UsGFDa91169bpgw8+UPPmzTOsZfz48Ro5cmSm6gYAALhSrpiBjI6Olru7u+rWrWu1Va5c2eVGlh07digpKUlFihSRn5+f9Tp06JAOHDhg9QsLC7PCoyQVL1483UxmVvXo0UPz58/X33//rYsXL+rTTz91CX03Uq1aNbm5uV2ztsjISLVt21alS5eWv7+/Ff6OHDlyzW3++uuvOnfunO69916X4zJnzhyX43K1l19+WQkJCdbr6NGjmR4HAADI33LFDGRmJCUlqXjx4lq9enW6ZVcGzYIFC7osczgcSk1NzZYa2rZtK09PT3399dfy8PBQcnKyHnvssUyvf73azp49q4iICEVERGjevHkKCgrSkSNHFBERoYsXL15zm0lJSZKk7777TiVKlHBZljZTmxFPT8/rLgcAALiWXBEgK1eurEuXLikyMtI6PRwTE+PyuJo6deroxIkTcnd3V1hYWJb35eHhoZSUlCz1cXd3V+fOnTVz5kx5eHjoySeflLe3d5ZrudK+fft06tQpTZgwwbouc+vWrenqkuRSW9WqVeXp6akjR45c83Q1AABAdsoVAbJSpUpq1aqVevXqpffff1/u7u4aMGCASzhr2bKlGjZsqHbt2mnSpEmqWLGijh07pu+++04PP/yw6tWrl6l9hYWFadOmTYqNjXW5rvDqPklJSVq5cqXCw8Pl4+MjHx8fSVL37t2tazPXr1+fDaO/rHTp0vLw8NC0adPUu3dv7d69O90zMkNDQ+VwOLR48WLdf//98vb2lr+/vwYNGqSBAwcqNTVVTZo0UUJCgtavXy+n06nOnTtnW40AAABSLrkGUrp8Q0pISIiaN2+uRx55RD179lTRokWt5Q6HQ0uWLFGzZs3UtWtXVaxYUU8++aQOHz6s4ODgTO9n0KBBcnNzU9WqVa3TxFdr1KiRevfurSeeeEJBQUGaNGmStaxChQpq1KiRKleurAYNGtzcoK8QFBSkWbNm6YsvvlDVqlU1YcIEvfHGGy59SpQooZEjR+qll15ScHCw+vXrJ0kaPXq0XnvtNY0fP15VqlRRq1at9N1336lMmTLZVh8AAEAahzHG5HQR/yTGGFWoUEF9+vTRCy+8kNPlZJvExEQFBAQo/LnpcvPMntPyt1Lk68/kdAkAAOS4tL/fCQkJcjqdt22/ueIU9j/FyZMn9dlnn+nEiROZfvYjAABAXkOAtKFo0aK644479OGHHyowMNBlmZ+f3zXXW7p0qZo2bXqrywMAALgtCJA2XO9sf1RU1DWXXf14HQAAgH8yAmQ2KV++fE6XAAAAcFvkmruwAQAA8M9AgAQAAIAtBEgAAADYQoAEAACALQRIAAAA2EKABAAAgC0ESAAAANhCgAQAAIAtBEgAAADYQoAEAACALQRIAAAA2EKABAAAgC0ESAAAANhCgAQAAIAt7jldAHKXtWM6yOl05nQZAAAgF2MGEgAAALYQIAEAAGALARIAAAC2ECABAABgCwESAAAAthAgAQAAYAsBEgAAALYQIAEAAGALARIAAAC2ECABAABgCwESAAAAtvBd2HDR7NX5cvP0tr1e5OvP3IJqAABAbsQMJAAAAGwhQAIAAMAWAiQAAABsIUACAADAFgIkAAAAbCFAAgAAwBYCJAAAAGwhQAIAAMAWAiQAAABsIUACAADAFgIkAAAAbCFAAgAAwBYCJAAAAGwhQAIAAMAWAiQAAABsIUACAADAFgIkAAAAbCFAAgAAwBYCJAAAAGwhQGaz1atXy+FwKD4+PqdLAQAAuCUIkDepRYsWGjBgQE6XYQkLC9PkyZNzugwAAJCHESBzgYsXL+Z0CQAAAJlGgLwJXbp00Zo1azRlyhQ5HA45HA7FxsZKkiIjI1WvXj35+PioUaNGiomJsdYbMWKEatWqpRkzZqhMmTLy8vKSJMXHx6t79+4KCgqS0+nUv/71L+3YscNa78CBA3rooYcUHBwsPz8/1a9fXz/88IO1vEWLFjp8+LAGDhxo1QMAAJDdCJA3YcqUKWrYsKF69Oih48eP6/jx4ypVqpQk6ZVXXtGbb76prVu3yt3dXd26dXNZ99dff9VXX32lBQsWKCoqSpL0+OOPKy4uTkuXLlVkZKTq1Kmje+65R6dPn5YkJSUl6f7779fKlSu1fft2tWrVSm3bttWRI0ckSQsWLFDJkiU1atQoq55ruXDhghITE11eAAAAmeGe0wX8kwUEBMjDw0M+Pj4qVqyYJGnfvn2SpLFjx6p58+aSpJdeeklt2rTR33//bc02Xrx4UXPmzFFQUJAkad26ddq8ebPi4uLk6ekpSXrjjTe0cOFCffnll+rZs6fCw8MVHh5u7X/06NH6+uuv9e2336pfv34qXLiw3Nzc5O/vb9VzLePHj9fIkSOz94AAAIB8gRnIW6RmzZrWz8WLF5ckxcXFWW2hoaFWeJSkHTt2KCkpSUWKFJGfn5/1OnTokA4cOCDp8gzkoEGDVKVKFRUqVEh+fn6Kjo62ZiDtePnll5WQkGC9jh49mtWhAgCAfIYZyFukYMGC1s9p1yKmpqZabb6+vi79k5KSVLx4ca1evTrdtgoVKiRJGjRokFasWKE33nhD5cuXl7e3tx577LEs3YTj6elpzXQCAADYQYC8SR4eHkpJSbnp7dSpU0cnTpyQu7u7wsLCMuyzfv16denSRQ8//LCky6Ez7aad7K4HAADgWjiFfZPCwsK0adMmxcbG6s8//3SZZbSjZcuWatiwodq1a6fly5crNjZWGzZs0CuvvKKtW7dKkipUqGDddLNjxw499dRT6fYXFhamtWvX6vfff9eff/550+MDAAC4GgHyJg0aNEhubm6qWrWqgoKCsnQ9onT5NPeSJUvUrFkzde3aVRUrVtSTTz6pw4cPKzg4WJL01ltvKTAwUI0aNVLbtm0VERGhOnXquGxn1KhRio2NVbly5VyusQQAAMguDmOMyekikPMSExMVEBCg8Oemy83T2/b6ka8/cwuqAgAA15P29zshIUFOp/O27ZcZSAAAANhCgAQAAIAtBEgAAADYQoAEAACALQRIAAAA2EKABAAAgC0ESAAAANhCgAQAAIAtBEgAAADYQoAEAACALQRIAAAA2EKABAAAgC0ESAAAANhCgAQAAIAtBEgAAADYQoAEAACALQRIAAAA2EKABAAAgC3uOV0Acpe1YzrI6XTmdBkAACAXYwYSAAAAthAgAQAAYAsBEgAAALYQIAEAAGALN9FAkmSMkSQlJibmcCUAACCz0v5up/0dv10IkJAknTp1SpJUqlSpHK4EAADYdebMGQUEBNy2/REgIUkqXLiwJOnIkSO39RcwpyUmJqpUqVI6evRovnl8UX4cs8S4GXfelx/HLDHuI0eOyOFwKCQk5LbunwAJSVKBApcvhw0ICMhX/wHTOJ3OfDfu/DhmiXHnN/lx3PlxzFL+HXdO/d3mJhoAAADYQoAEAACALQRISJI8PT01fPhweXp65nQpt1V+HHd+HLPEuBl33pcfxywx7pwat8Pc7vu+AQAA8I/GDCQAAABsIUACAADAFgIkAAAAbCFAAgAAwBYCJPTuu+8qLCxMXl5eatCggTZv3pzTJWXa+PHjVb9+ffn7+6to0aJq166dYmJiXPr8/fff6tu3r4oUKSI/Pz89+uij+uOPP1z6HDlyRG3atJGPj4+KFi2qwYMH69KlSy59Vq9erTp16sjT01Ply5fXrFmzbvXwMm3ChAlyOBwaMGCA1ZZXx/3777/r6aefVpEiReTt7a0aNWpo69at1nJjjIYNG6bixYvL29tbLVu21P79+122cfr0aXXs2FFOp1OFChXSs88+q6SkJJc+O3fuVNOmTeXl5aVSpUpp0qRJt2V8V0tJSdFrr72mMmXKyNvbW+XKldPo0aNdvvc2L4x57dq1atu2rUJCQuRwOLRw4UKX5bdzjF988YUqV64sLy8v1ahRQ0uWLMn28aa53riTk5M1dOhQ1ahRQ76+vgoJCdEzzzyjY8eOuWwjr437ar1795bD4dDkyZNd2vPquKOjo/Xggw8qICBAvr6+ql+/vo4cOWItzzWf7Qb52meffWY8PDzMf//7X7Nnzx7To0cPU6hQIfPHH3/kdGmZEhERYWbOnGl2795toqKizP33329Kly5tkpKSrD69e/c2pUqVMitXrjRbt241d911l2nUqJG1/NKlS6Z69eqmZcuWZvv27WbJkiXmjjvuMC+//LLV5+DBg8bHx8e88MILZu/evWbatGnGzc3NLFu27LaONyObN282YWFhpmbNmub555+32vPiuE+fPm1CQ0NNly5dzKZNm8zBgwfN999/b3799Verz4QJE0xAQIBZuHCh2bFjh3nwwQdNmTJlzPnz560+rVq1MuHh4ebnn382P/30kylfvrzp0KGDtTwhIcEEBwebjh07mt27d5v58+cbb29v88EHH9zW8RpjzNixY02RIkXM4sWLzaFDh8wXX3xh/Pz8zJQpU6w+eWHMS5YsMa+88opZsGCBkWS+/vprl+W3a4zr1683bm5uZtKkSWbv3r3m1VdfNQULFjS7du267eOOj483LVu2NJ9//rnZt2+f2bhxo7nzzjtN3bp1XbaR18Z9pQULFpjw8HATEhJi3n77bZdleXHcv/76qylcuLAZPHiw2bZtm/n111/NN9984/I3Obd8thMg87k777zT9O3b13qfkpJiQkJCzPjx43OwqqyLi4szksyaNWuMMZc/gAsWLGi++OILq090dLSRZDZu3GiMufwfukCBAubEiRNWn/fff984nU5z4cIFY4wxQ4YMMdWqVXPZ1xNPPGEiIiJu9ZCu68yZM6ZChQpmxYoVpnnz5laAzKvjHjp0qGnSpMk1l6empppixYqZ119/3WqLj483np6eZv78+cYYY/bu3WskmS1btlh9li5dahwOh/n999+NMca89957JjAw0DoOafuuVKlSdg/phtq0aWO6devm0vbII4+Yjh07GmPy5piv/sN6O8fYvn1706ZNG5d6GjRoYHr16pWtY8zI9YJUms2bNxtJ5vDhw8aYvD3u3377zZQoUcLs3r3bhIaGugTIvDruJ554wjz99NPXXCc3fbZzCjsfu3jxoiIjI9WyZUurrUCBAmrZsqU2btyYg5VlXUJCgiSpcOHCkqTIyEglJye7jLFy5coqXbq0NcaNGzeqRo0aCg4OtvpEREQoMTFRe/bssfpcuY20Pjl9nPr27as2bdqkqy2vjvvbb79VvXr19Pjjj6to0aKqXbu2PvroI2v5oUOHdOLECZeaAwIC1KBBA5dxFypUSPXq1bP6tGzZUgUKFNCmTZusPs2aNZOHh4fVJyIiQjExMfrrr79u9TBdNGrUSCtXrtQvv/wiSdqxY4fWrVun1q1bS8qbY77a7Rxjbvudv1pCQoIcDocKFSokKe+OOzU1VZ06ddLgwYNVrVq1dMvz4rhTU1P13XffqWLFioqIiFDRokXVoEEDl9PcuemznQCZj/35559KSUlx+SWTpODgYJ04cSKHqsq61NRUDRgwQI0bN1b16tUlSSdOnJCHh4f1YZvmyjGeOHEiw2OQtux6fRITE3X+/PlbMZwb+uyzz7Rt2zaNHz8+3bK8Ou6DBw/q/fffV4UKFfT999/r3//+t/r376/Zs2e71H293+kTJ06oaNGiLsvd3d1VuHBhW8fmdnnppZf05JNPqnLlyipYsKBq166tAQMGqGPHji715KUxX+12jvFafXL6GEiXr30bOnSoOnToIKfTKSnvjnvixIlyd3dX//79M1yeF8cdFxenpKQkTZgwQa1atdLy5cv18MMP65FHHtGaNWusenPLZ7u77RECuVTfvn21e/durVu3LqdLueWOHj2q559/XitWrJCXl1dOl3PbpKamql69eho3bpwkqXbt2tq9e7emT5+uzp0753B1t8b//vc/zZs3T59++qmqVaumqKgoDRgwQCEhIXl2zEgvOTlZ7du3lzFG77//fk6Xc0tFRkZqypQp2rZtmxwOR06Xc9ukpqZKkh566CENHDhQklSrVi1t2LBB06dPV/PmzXOyvHSYgczH7rjjDrm5uaW7e+uPP/5QsWLFcqiqrOnXr58WL16sVatWqWTJklZ7sWLFdPHiRcXHx7v0v3KMxYoVy/AYpC27Xh+n0ylvb+/sHs4NRUZGKi4uTnXq1JG7u7vc3d21Zs0aTZ06Ve7u7goODs6T4y5evLiqVq3q0lalShXrDsW0uq/3O12sWDHFxcW5LL906ZJOnz5t69jcLoMHD7ZmIWvUqKFOnTpp4MCB1sxzXhzz1W7nGK/VJyePQVp4PHz4sFasWGHNPkp5c9w//fST4uLiVLp0aevz7fDhw3rxxRcVFhZm1ZvXxn3HHXfI3d39hp9xueWznQCZj3l4eKhu3bpauXKl1ZaamqqVK1eqYcOGOVhZ5hlj1K9fP3399df68ccfVaZMGZfldevWVcGCBV3GGBMToyNHjlhjbNiwoXbt2uXyYZT2IZ32H7lhw4Yu20jrk1PH6Z577tGuXbsUFRVlverVq6eOHTtaP+fFcTdu3DjdY5p++eUXhYaGSpLKlCmjYsWKudScmJioTZs2uYw7Pj5ekZGRVp8ff/xRqampatCggdVn7dq1Sk5OtvqsWLFClSpVUmBg4C0bX0bOnTunAgVcP6rd3Nys2Yq8OOar3c4x5rbf+bTwuH//fv3www8qUqSIy/K8OO5OnTpp586dLp9vISEhGjx4sL7//nur3rw2bg8PD9WvX/+6n3G56m9apm+3QZ702WefGU9PTzNr1iyzd+9e07NnT1OoUCGXu7dys3//+98mICDArF692hw/ftx6nTt3zurTu3dvU7p0afPjjz+arVu3moYNG5qGDRtay9MeeXDfffeZqKgos2zZMhMUFJThIw8GDx5soqOjzbvvvptrHuOT5sq7sI3Jm+PevHmzcXd3N2PHjjX79+838+bNMz4+Pmbu3LlWnwkTJphChQqZb775xuzcudM89NBDGT7upXbt2mbTpk1m3bp1pkKFCi6P/4iPjzfBwcGmU6dOZvfu3eazzz4zPj4+OfIYn86dO5sSJUpYj/FZsGCBueOOO8yQIUOsPnlhzGfOnDHbt28327dvN5LMW2+9ZbZv327dbXy7xrh+/Xrj7u5u3njjDRMdHW2GDx9+Sx/rcr1xX7x40Tz44IOmZMmSJioqyuUz7so7i/PauDNy9V3YeXXcCxYsMAULFjQffvih2b9/v/V4nZ9++snaRm75bCdAwkybNs2ULl3aeHh4mDvvvNP8/PPPOV1SpknK8DVz5kyrz/nz502fPn1MYGCg8fHxMQ8//LA5fvy4y3ZiY2NN69atjbe3t7njjjvMiy++aJKTk136rFq1ytSqVct4eHiYsmXLuuwjN7g6QObVcS9atMhUr17deHp6msqVK5sPP/zQZXlqaqp57bXXTHBwsPH09DT33HOPiYmJcelz6tQp06FDB+Pn52ecTqfp2rWrOXPmjEufHTt2mCZNmhhPT09TokQJM2HChFs+towkJiaa559/3pQuXdp4eXmZsmXLmldeecUlQOSFMa9atSrD/8udO3c2xtzeMf7vf/8zFStWNB4eHqZatWrmu+++y5FxHzp06JqfcatWrcqz485IRgEyr477448/NuXLlzdeXl4mPDzcLFy40GUbueWz3WHMFV9nAAAAANwA10ACAADAFgIkAAAAbCFAAgAAwBYCJAAAAGwhQAIAAMAWAiQAAABsIUACAADAFgIkAGST2NhYORwORUVF5XQpln379umuu+6Sl5eXatWqldPlAMgjCJAA8owuXbrI4XBowoQJLu0LFy6Uw+HIoapy1vDhw+Xr66uYmJh0332bpkuXLmrXrl227TMsLEyTJ0/Otu0ByH0IkADyFC8vL02cOFF//fVXTpeSbS5evJjldQ8cOKAmTZooNDRURYoUycaqAORnBEgAeUrLli1VrFgxjR8//pp9RowYke507uTJkxUWFma9T5uVGzdunIKDg1WoUCGNGjVKly5d0uDBg1W4cGGVLFlSM2fOTLf9ffv2qVGjRvLy8lL16tW1Zs0al+W7d+9W69at5efnp+DgYHXq1El//vmntbxFixbq16+fBgwYoDvuuEMREREZjiM1NVWjRo1SyZIl5enpqVq1amnZsmXWcofDocjISI0aNUoOh0MjRoy4zpH7Py1atFD//v01ZMgQFS5cWMWKFXNZ1xijESNGqHTp0vL09FRISIj69+9vrXv48GENHDhQDofDmvk9deqUOnTooBIlSsjHx0c1atTQ/Pnzbe1XkuLj49WrVy8FBwdbx3fx4sXW8nXr1qlp06by9vZWqVKl1L9/f509e9Za/t5776lChQry8vJScHCwHnvssUwdEwCuCJAA8hQ3NzeNGzdO06ZN02+//XZT2/rxxx917NgxrV27Vm+99ZaGDx+uBx54QIGBgdq0aZN69+6tXr16pdvP4MGD9eKLL2r79u1q2LCh2rZtq1OnTkm6HID+9a9/qXbt2tq6dauWLVumP/74Q+3bt3fZxuzZs+Xh4aH169dr+vTpGdY3ZcoUvfnmm3rjjTe0c+dORURE6MEHH9T+/fslScePH1e1atX04osv6vjx4xo0aFCmxz579mz5+vpq06ZNmjRpkkaNGqUVK1ZIkr766iu9/fbb+uCDD7R//34tXLhQNWrUkCQtWLBAJUuW1KhRo3T8+HEdP35ckvT333+rbt26+u6777R792717NlTnTp10ubNmzO939TUVLVu3Vrr16/X3LlztXfvXk2YMEFubm6SLs+2tmrVSo8++qh27typzz//XOvWrVO/fv0kSVu3blX//v01atQoxcTEaNmyZWrWrFmmjwmAKxgAyCM6d+5sHnroIWOMMXfddZfp1q2bMcaYr7/+2lz5cTd8+HATHh7usu7bb79tQkNDXbYVGhpqUlJSrLZKlSqZpk2bWu8vXbpkfH19zfz5840xxhw6dMhIMhMmTLD6JCcnm5IlS5qJEycaY4wZPXq0ue+++1z2ffToUSPJxMTEGGOMad68ualdu/YNxxsSEmLGjh3r0la/fn3Tp08f6314eLgZPnz4dbdz5XFL23+TJk3SbXfo0KHGGGPefPNNU7FiRXPx4sUMtxcaGmrefvvtG9bfpk0b8+KLL2Z6v99//70pUKCAdZyu9uyzz5qePXu6tP3000+mQIEC5vz58+arr74yTqfTJCYm3rA2ANfHDCSAPGnixImaPXu2oqOjs7yNatWqqUCB//uYDA4OtmbapMuznUWKFFFcXJzLeg0bNrR+dnd3V7169aw6duzYoVWrVsnPz896Va5cWdLlGbQ0devWvW5tiYmJOnbsmBo3buzS3rhx45sac5qaNWu6vC9evLg1zscff1znz59X2bJl1aNHD3399de6dOnSdbeXkpKi0aNHq0aNGipcuLD8/Pz0/fff68iRI5neb1RUlEqWLKmKFStmuI8dO3Zo1qxZLsc2IiJCqampOnTokO69916FhoaqbNmy6tSpk+bNm6dz587ZOi4ALiNAAsiTmjVrpoiICL388svplhUoUEDGGJe25OTkdP0KFizo8t7hcGTYlpqamum6kpKS1LZtW0VFRbm89u/f73I61dfXN9PbvBWuN85SpUopJiZG7733nry9vdWnTx81a9Ysw2OY5vXXX9eUKVM0dOhQrVq1SlFRUYqIiEh3g9D19uvt7X3dmpOSktSrVy+X47pjxw7t379f5cqVk7+/v7Zt26b58+erePHiGjZsmMLDwxUfH5/ZwwLg/yNAAsizJkyYoEWLFmnjxo0u7UFBQTpx4oRLiMzOZzf+/PPP1s+XLl1SZGSkqlSpIkmqU6eO9uzZo7CwMJUvX97lZSc0Op1OhYSEaP369S7t69evV9WqVbNnINfh7e2ttm3baurUqVq9erU2btyoXbt2SZI8PDyUkpKSrq6HHnpITz/9tMLDw1W2bFn98ssvtvZZs2ZN/fbbb9dcr06dOtq7d2+641q+fHl5eHhIujwj3LJlS02aNEk7d+5UbGysfvzxxywcASB/I0ACyLNq1Kihjh07aurUqS7tLVq00MmTJzVp0iQdOHBA7777rpYuXZpt+3333Xf19ddfa9++ferbt6/++usvdevWTZLUt29fnT59Wh06dNCWLVt04MABff/99+ratWu60HUjgwcP1sSJE/X5558rJiZGL730kqKiovT8889n21gyMmvWLH388cfavXu3Dh48qLlz58rb21uhoaGSLj8Hcu3atfr999+tu8srVKigFStWaMOGDYqOjlavXr30xx9/2Npv8+bN1axZMz366KNasWKFDh06pKVLl1p3ng8dOlQbNmxQv379rFndb775xrqJZvHixZo6daqioqJ0+PBhzZkzR6mpqapUqVI2Hh0gfyBAAsjTRo0ale4Uc5UqVfTee+/p3XffVXh4uDZv3mzrDuUbmTBhgiZMmKDw8HCtW7dO3377re644w5JsmYNU1JSdN9996lGjRoaMGCAChUq5HK9ZWb0799fL7zwgl588UXVqFFDy5Yt07fffqsKFSpk21gyUqhQIX300Udq3LixatasqR9++EGLFi2ynjM5atQoxcbGqly5cgoKCpIkvfrqq6pTp44iIiLUokULFStWLEsPL//qq69Uv359dejQQVWrVtWQIUOs4F2zZk2tWbNGv/zyi5o2baratWtr2LBhCgkJsepesGCB/vWvf6lKlSqaPn265s+fr2rVqmXPgQHyEYe5+kIgAAAA4DqYgQQAAIAtBEgAAADYQoAEAACALQRIAAAA2EKABAAAgC0ESAAAANhCgAQAAIAtBEgAAADYQoAEAACALQRIAAAA2EKABAAAgC0ESAAAANjy/wDdCHtNBKarsQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "labels = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "train_labels = df_train[labels].sum().sort_values(ascending=False)\n",
    "sns.barplot(x = train_labels.values, y = train_labels.index)\n",
    "plt.title('Training Data Label Distribution')\n",
    "plt.xlabel('Number of Instances')\n",
    "plt.ylabel('Labels')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data type counts in 'comment_text':\n",
      "comment_text\n",
      "<class 'str'>    159469\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Number of non-string 'comment_text' entries: 0\n",
      "Number of non-string 'comment_text' entries after cleaning: 0\n",
      "\n",
      "Data type counts in 'comment_text':\n",
      "comment_text\n",
      "<class 'str'>    152258\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Number of non-string 'comment_text' entries: 0\n",
      "Number of non-string 'comment_text' entries after cleaning: 0\n"
     ]
    }
   ],
   "source": [
    "# For train data\n",
    "# Get a count of different data types within 'comment_text'\n",
    "print(\"\\nData type counts in 'comment_text':\")\n",
    "print(df_train['comment_text'].apply(type).value_counts())\n",
    "\n",
    "# Display the number of non-string entries\n",
    "non_string_count = df_train[~df_train['comment_text'].apply(lambda x: isinstance(x, str))].shape[0]\n",
    "print(f\"\\nNumber of non-string 'comment_text' entries: {non_string_count}\")\n",
    "\n",
    "# Remove rows where 'comment_text' is not a string\n",
    "train_df_cleaned = df_train[df_train['comment_text'].apply(lambda x: isinstance(x, str))].reset_index(drop=True)\n",
    "\n",
    "# Confirm removal\n",
    "non_string_count_after = train_df_cleaned[~train_df_cleaned['comment_text'].apply(lambda x: isinstance(x, str))].shape[0]\n",
    "print(f\"Number of non-string 'comment_text' entries after cleaning: {non_string_count_after}\")\n",
    "\n",
    "\n",
    "# For test data\n",
    "# Get a count of different data types within 'comment_text'\n",
    "print(\"\\nData type counts in 'comment_text':\")\n",
    "print(df_test['comment_text'].apply(type).value_counts())\n",
    "\n",
    "# Display the number of non-string entries\n",
    "non_string_count = df_test[~df_test['comment_text'].apply(lambda x: isinstance(x, str))].shape[0]\n",
    "print(f\"\\nNumber of non-string 'comment_text' entries: {non_string_count}\")\n",
    "\n",
    "# Remove rows where 'comment_text' is not a string\n",
    "df_test_cleaned = df_test[df_test['comment_text'].apply(lambda x: isinstance(x, str))].reset_index(drop=True)\n",
    "\n",
    "# Confirm removal\n",
    "non_string_count_after = df_test_cleaned[~df_test_cleaned['comment_text'].apply(lambda x: isinstance(x, str))].shape[0]\n",
    "print(f\"Number of non-string 'comment_text' entries after cleaning: {non_string_count_after}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 127575\n",
      "Validation samples: 31894\n"
     ]
    }
   ],
   "source": [
    "# Splitting Data to train and validation\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit\n",
    "\n",
    "X = train_df_cleaned['comment_text'].values\n",
    "y = train_df_cleaned[labels].values\n",
    "\n",
    "mss = MultilabelStratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=41)\n",
    "\n",
    "for train_index, val_index in mss.split(X,y):\n",
    "    X_train, X_val = X[train_index], X[val_index]\n",
    "    y_train, y_val = y[train_index], y[val_index]\n",
    "print(f'Training samples: {len(X_train)}')\n",
    "print(f'Validation samples: {len(X_val)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing: 100%|██████████| 128/128 [01:32<00:00,  1.38it/s]\n",
      "Tokenizing: 100%|██████████| 32/32 [00:23<00:00,  1.39it/s]\n",
      "Tokenizing: 100%|██████████| 153/153 [01:41<00:00,  1.50it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "import re\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# Define maximum sequence length\n",
    "MAX_LENGTH = 128\n",
    "\n",
    "# def preprocess_text(text):\n",
    "#     # Remove HTML tags\n",
    "#     text = re.sub(r'<.*?>', '', text)\n",
    "#     # Remove URLs\n",
    "#     text = re.sub(r'http\\S+', '', text)\n",
    "#     # Remove special characters (optional)\n",
    "#     text = re.sub(r'[^A-Za-z0-9\\s]+', '', text)\n",
    "#     return text\n",
    "\n",
    "# Apply preprocessing\n",
    "X_train = [text for text in X_train]\n",
    "X_val = [text for text in X_val]\n",
    "test_comments = [text for text in df_test_cleaned['comment_text'].values]\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "MAX_LENGTH = 128\n",
    "BATCH_SIZE = 1000  # Adjust based on your system's memory\n",
    "\n",
    "def batch_tokenize(texts, tokenizer, max_length, batch_size):\n",
    "    encodings = {\n",
    "        'input_ids': [],\n",
    "        'attention_mask': []\n",
    "    }\n",
    "    \n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Tokenizing\"):\n",
    "        batch_texts = texts[i:i+batch_size]\n",
    "        batch_encodings = tokenizer(\n",
    "            batch_texts,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        encodings['input_ids'].append(batch_encodings['input_ids'])\n",
    "        encodings['attention_mask'].append(batch_encodings['attention_mask'])\n",
    "    \n",
    "    # Concatenate all batches\n",
    "    encodings['input_ids'] = torch.cat(encodings['input_ids'])\n",
    "    encodings['attention_mask'] = torch.cat(encodings['attention_mask'])\n",
    "    \n",
    "    return encodings\n",
    "\n",
    "# Tokenize training and validation data\n",
    "train_encodings = batch_tokenize(X_train, tokenizer, MAX_LENGTH, BATCH_SIZE)\n",
    "val_encodings = batch_tokenize(X_val, tokenizer, MAX_LENGTH, BATCH_SIZE)\n",
    "test_encodings = batch_tokenize(test_comments, tokenizer, MAX_LENGTH, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce RTX 3060 Ti\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())  # Should return True\n",
    "print(torch.cuda.get_device_name(0))  # Prints your GPU name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Weights: [1.8267286522083421, 3.9316335311711703, 2.3453971167444507, 5.123862690304606, 2.408905953466444, 4.056131052226914]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n",
      "Model is on device: cuda:0\n",
      "Example tensor is on device: cpu\n",
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hewzh\\Desktop\\Code\\Data Cleaning\\venv\\Lib\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Creating Pytorch Datasets\n",
    "import torch\n",
    "import torch.utils\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, roc_auc_score, average_precision_score\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import EarlyStoppingCallback\n",
    "import numpy as np\n",
    "\n",
    "class ToxicDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels=None):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels  # None for test data\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        if self.labels is not None:\n",
    "            item['labels'] = torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings['input_ids'])\n",
    "    \n",
    "train_dataset = ToxicDataset(train_encodings, y_train)\n",
    "val_dataset = ToxicDataset(val_encodings, y_val)\n",
    "test_dataset = ToxicDataset(test_encodings)  # No labels\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "class_weights = {}\n",
    "\n",
    "for i,label in enumerate(labels):\n",
    "    cw = compute_class_weight(\n",
    "        class_weight=\"balanced\",\n",
    "        classes= np.array([0,1]),\n",
    "        y=y_train[:,i]\n",
    "    )\n",
    "    class_weights[label] = np.log1p(cw[1])\n",
    "\n",
    "class_weights_list = [class_weights[label] for label in labels]\n",
    "print(\"Class Weights:\", class_weights_list)\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels = len(labels), problem_type=\"multi_label_classification\")\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "# 7. Define Metrics\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions\n",
    "    probs = torch.sigmoid(torch.tensor(preds)).numpy()\n",
    "    binary_preds = (probs >= 0.5).astype(int)\n",
    "    \n",
    "    f1 = f1_score(labels, binary_preds, average='macro', zero_division=0)\n",
    "    precision = precision_score(labels, binary_preds, average='macro', zero_division=0)\n",
    "    recall = recall_score(labels, binary_preds, average='macro', zero_division=0)\n",
    "    auc = roc_auc_score(labels, probs, average='macro')\n",
    "    avg_precision = average_precision_score(labels, probs, average='macro')\n",
    "    \n",
    "    return {\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'auc': auc,\n",
    "        'average_precision': avg_precision\n",
    "    }\n",
    "\n",
    "# 8. Set Up Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',                   # Output directory\n",
    "    num_train_epochs=7,                       # Number of training epochs\n",
    "    per_device_train_batch_size=32,           # Batch size per device during training\n",
    "    per_device_eval_batch_size=32,            # Batch size for evaluation\n",
    "    learning_rate=2e-5,  # Lower learning rate for fine-tuning\n",
    "    weight_decay=0.01,                        # Strength of weight decay\n",
    "    logging_dir='./logs',                     # Directory for storing logs\n",
    "    logging_steps=10,                         # Log every 10 steps\n",
    "    evaluation_strategy=\"epoch\",              # Evaluate at the end of each epoch\n",
    "    save_strategy=\"epoch\",                    # Save checkpoint at each epoch\n",
    "    load_best_model_at_end=True,              # Load the best model when finished training\n",
    "    metric_for_best_model=\"f1\",               # Best model is determined by the F1 score\n",
    "    greater_is_better=True,                   # Whether a higher metric score is better\n",
    "    save_total_limit=2,                       # Limit the total amount of checkpoints\n",
    "    seed=42,                                   # Random seed for reproducibility\n",
    "    fp16=True,                                 # Enable mixed precision training (requires compatible hardware)\n",
    "    report_to=\"tensorboard\",\n",
    "    max_grad_norm= 1.0\n",
    "    \n",
    ")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class WeightedBCELoss(nn.Module):\n",
    "    def __init__(self, class_weights):\n",
    "        super(WeightedBCELoss, self).__init__()\n",
    "        # Convert class_weights to a tensor; don't move to device here\n",
    "        self.class_weights = torch.tensor(class_weights).float()\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        # Ensure class_weights are on the same device as logits\n",
    "        self.class_weights = self.class_weights.to(logits.device)\n",
    "        # Compute Binary Cross-Entropy with Logits Loss without reduction\n",
    "        bce_loss = nn.BCEWithLogitsLoss(reduction='none')(logits, targets)\n",
    "        # Apply class weights\n",
    "        weighted_loss = bce_loss * self.class_weights\n",
    "        # Return the mean loss\n",
    "        return weighted_loss.mean()\n",
    "\n",
    "\n",
    "class WeightedTrainer(Trainer):\n",
    "    def __init__(self, *args, class_weights=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        if class_weights is None:\n",
    "            raise ValueError(\"class_weights must be provided\")\n",
    "        self.weighted_loss = WeightedBCELoss(class_weights).to(self.model.device)\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        # Move inputs to the same device as the model\n",
    "        inputs = {key: val.to(model.device) for key, val in inputs.items() if isinstance(val, torch.Tensor)}\n",
    "        \n",
    "        labels = inputs.pop(\"labels\")  # Extract labels\n",
    "        outputs = model(**inputs)      # Forward pass\n",
    "        logits = outputs.logits        # Get logits from model outputs\n",
    "        \n",
    "        # Compute weighted loss\n",
    "        loss = self.weighted_loss(logits, labels)\n",
    "        # Debugging: Print device info\n",
    "        # print(f\"Logits device: {logits.device}\")\n",
    "        # print(f\"Labels device: {labels.device}\")\n",
    "        # print(f\"Class Weights device: {self.weighted_loss.class_weights.device}\")\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "# Check if CUDA is available\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "\n",
    "# Check model's device\n",
    "print(f\"Model is on device: {next(model.parameters()).device}\")\n",
    "\n",
    "# Example tensor\n",
    "example_tensor = torch.tensor([1, 2, 3])\n",
    "print(f\"Example tensor is on device: {example_tensor.device}\")\n",
    "\n",
    "# Define device\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cc4203d06284288ac07c3cd9df48603",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27909 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hewzh\\AppData\\Local\\Temp\\ipykernel_5460\\560306528.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.2626, 'grad_norm': 10.384536743164062, 'learning_rate': 9.996775233795551e-06, 'epoch': 0.0}\n",
      "{'loss': 1.8668, 'grad_norm': 10.22737979888916, 'learning_rate': 9.99319216023505e-06, 'epoch': 0.01}\n",
      "{'loss': 1.4824, 'grad_norm': 7.818345546722412, 'learning_rate': 9.98960908667455e-06, 'epoch': 0.01}\n",
      "{'loss': 1.1957, 'grad_norm': 6.531576633453369, 'learning_rate': 9.98602601311405e-06, 'epoch': 0.01}\n",
      "{'loss': 0.9543, 'grad_norm': 4.751241683959961, 'learning_rate': 9.982442939553549e-06, 'epoch': 0.01}\n",
      "{'loss': 0.7525, 'grad_norm': 3.154741048812866, 'learning_rate': 9.97885986599305e-06, 'epoch': 0.02}\n",
      "{'loss': 0.6331, 'grad_norm': 2.6958231925964355, 'learning_rate': 9.97527679243255e-06, 'epoch': 0.02}\n",
      "{'loss': 0.5732, 'grad_norm': 2.519179105758667, 'learning_rate': 9.97169371887205e-06, 'epoch': 0.02}\n",
      "{'loss': 0.4526, 'grad_norm': 1.5121499300003052, 'learning_rate': 9.968110645311549e-06, 'epoch': 0.02}\n",
      "{'loss': 0.461, 'grad_norm': 1.6681212186813354, 'learning_rate': 9.964527571751049e-06, 'epoch': 0.03}\n",
      "{'loss': 0.4028, 'grad_norm': 1.1517534255981445, 'learning_rate': 9.96094449819055e-06, 'epoch': 0.03}\n",
      "{'loss': 0.408, 'grad_norm': 1.5680514574050903, 'learning_rate': 9.957361424630048e-06, 'epoch': 0.03}\n",
      "{'loss': 0.3083, 'grad_norm': 1.4182932376861572, 'learning_rate': 9.953778351069548e-06, 'epoch': 0.03}\n",
      "{'loss': 0.3776, 'grad_norm': 1.9216582775115967, 'learning_rate': 9.950195277509049e-06, 'epoch': 0.04}\n",
      "{'loss': 0.2923, 'grad_norm': 1.2873961925506592, 'learning_rate': 9.946612203948547e-06, 'epoch': 0.04}\n",
      "{'loss': 0.2983, 'grad_norm': 1.9945160150527954, 'learning_rate': 9.943029130388048e-06, 'epoch': 0.04}\n",
      "{'loss': 0.2609, 'grad_norm': 1.2764191627502441, 'learning_rate': 9.939446056827548e-06, 'epoch': 0.04}\n",
      "{'loss': 0.2853, 'grad_norm': 1.7250267267227173, 'learning_rate': 9.935862983267047e-06, 'epoch': 0.05}\n",
      "{'loss': 0.2922, 'grad_norm': 3.869218111038208, 'learning_rate': 9.932279909706547e-06, 'epoch': 0.05}\n",
      "{'loss': 0.2252, 'grad_norm': 0.8851428031921387, 'learning_rate': 9.928696836146047e-06, 'epoch': 0.05}\n",
      "{'loss': 0.2305, 'grad_norm': 2.062525749206543, 'learning_rate': 9.925113762585546e-06, 'epoch': 0.05}\n",
      "{'loss': 0.1794, 'grad_norm': 1.1673927307128906, 'learning_rate': 9.921530689025046e-06, 'epoch': 0.06}\n",
      "{'loss': 0.231, 'grad_norm': 1.6016943454742432, 'learning_rate': 9.917947615464547e-06, 'epoch': 0.06}\n",
      "{'loss': 0.2366, 'grad_norm': 1.688332438468933, 'learning_rate': 9.914364541904045e-06, 'epoch': 0.06}\n",
      "{'loss': 0.2556, 'grad_norm': 2.769545793533325, 'learning_rate': 9.910781468343546e-06, 'epoch': 0.06}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 13\u001b[0m\n\u001b[0;32m      3\u001b[0m trainer \u001b[38;5;241m=\u001b[39m WeightedTrainer(\n\u001b[0;32m      4\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,                             \u001b[38;5;66;03m# The instantiated Transformers model to be trained\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,                      \u001b[38;5;66;03m# Training arguments, defined above\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      9\u001b[0m     class_weights\u001b[38;5;241m=\u001b[39mclass_weights_list\n\u001b[0;32m     10\u001b[0m )\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# 10. Start Training\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\hewzh\\Desktop\\Code\\Data Cleaning\\venv\\Lib\\site-packages\\transformers\\trainer.py:2164\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2162\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2163\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2165\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2166\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2169\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\hewzh\\Desktop\\Code\\Data Cleaning\\venv\\Lib\\site-packages\\transformers\\trainer.py:2524\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2517\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2518\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[0;32m   2519\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   2520\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[0;32m   2521\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[0;32m   2522\u001b[0m )\n\u001b[0;32m   2523\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[1;32m-> 2524\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2526\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2527\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2528\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m   2529\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   2530\u001b[0m ):\n\u001b[0;32m   2531\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2532\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32mc:\\Users\\hewzh\\Desktop\\Code\\Data Cleaning\\venv\\Lib\\site-packages\\transformers\\trainer.py:3687\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   3685\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m   3686\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 3687\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3688\u001b[0m     \u001b[38;5;66;03m# Finally we need to normalize the loss for reporting\u001b[39;00m\n\u001b[0;32m   3689\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m num_items_in_batch \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\hewzh\\Desktop\\Code\\Data Cleaning\\venv\\Lib\\site-packages\\accelerate\\accelerator.py:2244\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[1;34m(self, loss, **kwargs)\u001b[0m\n\u001b[0;32m   2242\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m   2243\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 2244\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2245\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m learning_rate \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_lomo_optimizer:\n\u001b[0;32m   2246\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n",
      "File \u001b[1;32mc:\\Users\\hewzh\\Desktop\\Code\\Data Cleaning\\venv\\Lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\hewzh\\Desktop\\Code\\Data Cleaning\\venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\hewzh\\Desktop\\Code\\Data Cleaning\\venv\\Lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "#8. Trainer arguments\n",
    "trainer = WeightedTrainer(\n",
    "    model=model,                             # The instantiated Transformers model to be trained\n",
    "    args=training_args,                      # Training arguments, defined above\n",
    "    train_dataset=train_dataset,             # Training dataset\n",
    "    eval_dataset=val_dataset,                # Evaluation dataset\n",
    "    compute_metrics=compute_metrics,         # Function to compute metrics\n",
    "    class_weights=class_weights_list\n",
    ")\n",
    "\n",
    "# 10. Start Training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hewzh\\AppData\\Local\\Temp\\ipykernel_5460\\1797872413.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bc0881b62064d36a356b4e740e6e1f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/997 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results:\n",
      "eval_loss: 0.1434\n",
      "eval_f1: 0.3841\n",
      "eval_precision: 0.3605\n",
      "eval_recall: 0.4122\n",
      "eval_auc: 0.9664\n",
      "eval_average_precision: 0.5360\n",
      "eval_runtime: 42.4806\n",
      "eval_samples_per_second: 750.7890\n",
      "eval_steps_per_second: 23.4700\n",
      "epoch: 0.1756\n"
     ]
    }
   ],
   "source": [
    "# 11. Evaluate Model\n",
    "results = trainer.evaluate()\n",
    "print(\"Evaluation Results:\")\n",
    "for key, value in results.items():\n",
    "    print(f\"{key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./final_model2\\\\tokenizer_config.json',\n",
       " './final_model2\\\\special_tokens_map.json',\n",
       " './final_model2\\\\vocab.txt',\n",
       " './final_model2\\\\added_tokens.json')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.save_model(\"./final_model3\")  # Saves the model and tokenizer\n",
    "tokenizer.save_pretrained(\"./final_model3\")  # Optional: Save tokenizer separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n",
      "Model is on device: cuda:0\n",
      "Example tensor is on device: cpu\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Assuming you are using Hugging Face's Trainer\n",
    "outputs = trainer.predict(val_dataset)  # Evaluate on the validation dataset\n",
    "\n",
    "# Extract predictions and true labels\n",
    "predictions = outputs.predictions  # Raw logits\n",
    "true_labels = outputs.label_ids     # True labels from the dataset\n",
    "\n",
    "# Convert logits to probabilities\n",
    "probabilities = torch.sigmoid(torch.tensor(predictions)).numpy()\n",
    "\n",
    "# Convert probabilities to binary predictions (threshold = 0.5 by default)\n",
    "binary_predictions = (probabilities >= 0.5).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Compute confusion matrices for each label\n",
    "confusion_matrices = {}\n",
    "for i, label in enumerate(labels):\n",
    "    cm = confusion_matrix(true_labels[:, i], binary_predictions[:, i])\n",
    "    confusion_matrices[label] = cm\n",
    "\n",
    "# Display confusion matrices\n",
    "for label, cm in confusion_matrices.items():\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=[\"Negative\", \"Positive\"], yticklabels=[\"Negative\", \"Positive\"])\n",
    "    plt.title(f\"Confusion Matrix for {label}\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.show()\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Compute metrics for each label\n",
    "for i, label in enumerate(labels):\n",
    "    precision = precision_score(true_labels[:, i], binary_predictions[:, i], zero_division=0)\n",
    "    recall = recall_score(true_labels[:, i], binary_predictions[:, i], zero_division=0)\n",
    "    f1 = f1_score(true_labels[:, i], binary_predictions[:, i], zero_division=0)\n",
    "    print(f\"Metrics for {label}: Precision={precision:.2f}, Recall={recall:.2f}, F1={f1:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# # Load tokenizer and model\n",
    "# tokenizer = BertTokenizer.from_pretrained('./final_model2')\n",
    "# model = BertForSequenceClassification.from_pretrained('./final_model2')\n",
    "# model.eval()  # Set to evaluation mode\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to 'test_predictions.csv'\n"
     ]
    }
   ],
   "source": [
    "# test_comments = df_test_cleaned['comment_text'].tolist()\n",
    "# # Define prediction function\n",
    "# def predict(text, tokenizer, model, device, max_length=128):\n",
    "#     inputs = tokenizer(\n",
    "#         text,\n",
    "#         return_tensors='pt',\n",
    "#         truncation=True,\n",
    "#         padding='max_length',\n",
    "#         max_length=max_length\n",
    "#     )\n",
    "#     # Move inputs to device\n",
    "#     inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         outputs = model(**inputs)\n",
    "#         logits = outputs.logits\n",
    "#         probabilities = torch.sigmoid(logits).cpu().numpy()[0]  # Move to CPU for numpy operations\n",
    "#     binary_preds = (probabilities >= 0.5).astype(int)\n",
    "#     return binary_preds, probabilities\n",
    "\n",
    "# # Perform predictions\n",
    "# predictions = []\n",
    "# probabilities = []\n",
    "\n",
    "# for comment in test_comments:\n",
    "#     binary_pred, prob = predict(comment, tokenizer, model, device)\n",
    "#     predictions.append(binary_pred)\n",
    "#     probabilities.append(prob)\n",
    "\n",
    "# # Convert predictions to DataFrame\n",
    "# labels = ['toxic', 'severe_toxic', 'obscene', 'insult', 'identity_hate', 'threat']\n",
    "# preds_df = pd.DataFrame(predictions, columns=labels)\n",
    "# probs_df = pd.DataFrame(probabilities, columns=[f\"{label}_prob\" for label in labels])\n",
    "\n",
    "# # Combine with test IDs\n",
    "# submission_df = pd.concat([df_test_cleaned['id'], preds_df, probs_df], axis=1)\n",
    "\n",
    "# # Save to CSV\n",
    "# submission_df.to_csv('test_predictions.csv', index=False)\n",
    "# print(\"Predictions saved to 'test_predictions.csv'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
