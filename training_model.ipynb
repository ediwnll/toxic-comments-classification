{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_train = pd.read_csv('train_preprocess.csv')\n",
    "df_test = pd.read_csv('test_preprocess.csv')\n",
    "\n",
    "print(df_train.head())\n",
    "print(df_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for null values\n",
    "print(df_train.isnull().sum())\n",
    "print(df_test.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.dropna(subset=['comment_text'])\n",
    "df_test = df_test.dropna(subset=['comment_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "labels = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "train_labels = df_train[labels].sum().sort_values(ascending=False)\n",
    "sns.barplot(x = train_labels.values, y = train_labels.index)\n",
    "plt.title('Training Data Label Distribution')\n",
    "plt.xlabel('Number of Instances')\n",
    "plt.ylabel('Labels')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For train data\n",
    "# Get a count of different data types within 'comment_text'\n",
    "print(\"\\nData type counts in 'comment_text':\")\n",
    "print(df_train['comment_text'].apply(type).value_counts())\n",
    "\n",
    "# Display the number of non-string entries\n",
    "non_string_count = df_train[~df_train['comment_text'].apply(lambda x: isinstance(x, str))].shape[0]\n",
    "print(f\"\\nNumber of non-string 'comment_text' entries: {non_string_count}\")\n",
    "\n",
    "# Remove rows where 'comment_text' is not a string\n",
    "train_df_cleaned = df_train[df_train['comment_text'].apply(lambda x: isinstance(x, str))].reset_index(drop=True)\n",
    "\n",
    "# Confirm removal\n",
    "non_string_count_after = train_df_cleaned[~train_df_cleaned['comment_text'].apply(lambda x: isinstance(x, str))].shape[0]\n",
    "print(f\"Number of non-string 'comment_text' entries after cleaning: {non_string_count_after}\")\n",
    "\n",
    "\n",
    "# For test data\n",
    "# Get a count of different data types within 'comment_text'\n",
    "print(\"\\nData type counts in 'comment_text':\")\n",
    "print(df_test['comment_text'].apply(type).value_counts())\n",
    "\n",
    "# Display the number of non-string entries\n",
    "non_string_count = df_test[~df_test['comment_text'].apply(lambda x: isinstance(x, str))].shape[0]\n",
    "print(f\"\\nNumber of non-string 'comment_text' entries: {non_string_count}\")\n",
    "\n",
    "# Remove rows where 'comment_text' is not a string\n",
    "df_test_cleaned = df_test[df_test['comment_text'].apply(lambda x: isinstance(x, str))].reset_index(drop=True)\n",
    "\n",
    "# Confirm removal\n",
    "non_string_count_after = df_test_cleaned[~df_test_cleaned['comment_text'].apply(lambda x: isinstance(x, str))].shape[0]\n",
    "print(f\"Number of non-string 'comment_text' entries after cleaning: {non_string_count_after}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting Data to train and validation\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit\n",
    "\n",
    "X = train_df_cleaned['comment_text'].values\n",
    "y = train_df_cleaned[labels].values\n",
    "\n",
    "mss = MultilabelStratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=41)\n",
    "\n",
    "for train_index, val_index in mss.split(X,y):\n",
    "    X_train, X_val = X[train_index], X[val_index]\n",
    "    y_train, y_val = y[train_index], y[val_index]\n",
    "print(f'Training samples: {len(X_train)}')\n",
    "print(f'Validation samples: {len(X_val)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "import re\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# Define maximum sequence length\n",
    "MAX_LENGTH = 128\n",
    "\n",
    "# def preprocess_text(text):\n",
    "#     # Remove HTML tags\n",
    "#     text = re.sub(r'<.*?>', '', text)\n",
    "#     # Remove URLs\n",
    "#     text = re.sub(r'http\\S+', '', text)\n",
    "#     # Remove special characters (optional)\n",
    "#     text = re.sub(r'[^A-Za-z0-9\\s]+', '', text)\n",
    "#     return text\n",
    "\n",
    "# Apply preprocessing\n",
    "X_train = [text for text in X_train]\n",
    "X_val = [text for text in X_val]\n",
    "test_comments = [text for text in df_test_cleaned['comment_text'].values]\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "MAX_LENGTH = 128\n",
    "BATCH_SIZE = 1000  # Adjust based on your system's memory\n",
    "\n",
    "def batch_tokenize(texts, tokenizer, max_length, batch_size):\n",
    "    encodings = {\n",
    "        'input_ids': [],\n",
    "        'attention_mask': []\n",
    "    }\n",
    "    \n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Tokenizing\"):\n",
    "        batch_texts = texts[i:i+batch_size]\n",
    "        batch_encodings = tokenizer(\n",
    "            batch_texts,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        encodings['input_ids'].append(batch_encodings['input_ids'])\n",
    "        encodings['attention_mask'].append(batch_encodings['attention_mask'])\n",
    "    \n",
    "    # Concatenate all batches\n",
    "    encodings['input_ids'] = torch.cat(encodings['input_ids'])\n",
    "    encodings['attention_mask'] = torch.cat(encodings['attention_mask'])\n",
    "    \n",
    "    return encodings\n",
    "\n",
    "# Tokenize training and validation data\n",
    "train_encodings = batch_tokenize(X_train, tokenizer, MAX_LENGTH, BATCH_SIZE)\n",
    "val_encodings = batch_tokenize(X_val, tokenizer, MAX_LENGTH, BATCH_SIZE)\n",
    "test_encodings = batch_tokenize(test_comments, tokenizer, MAX_LENGTH, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.is_available())  # Should return True\n",
    "print(torch.cuda.get_device_name(0))  # Prints your GPU name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Pytorch Datasets\n",
    "import torch.utils\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, roc_auc_score, average_precision_score\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import EarlyStoppingCallback\n",
    "import numpy as np\n",
    "\n",
    "class ToxicDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels=None):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels  # None for test data\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        if self.labels is not None:\n",
    "            item['labels'] = torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings['input_ids'])\n",
    "    \n",
    "train_dataset = ToxicDataset(train_encodings, y_train)\n",
    "val_dataset = ToxicDataset(val_encodings, y_val)\n",
    "test_dataset = ToxicDataset(test_encodings)  # No labels\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "class_weights = {}\n",
    "\n",
    "for i,label in enumerate(labels):\n",
    "    cw = compute_class_weight(\n",
    "        class_weight=\"balanced\",\n",
    "        classes= np.array([0,1]),\n",
    "        y=y_train[:,i]\n",
    "    )\n",
    "    class_weights[label] = np.log1p(cw[1])\n",
    "\n",
    "class_weights_list = [class_weights[label] for label in labels]\n",
    "print(\"Class Weights:\", class_weights_list)\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels = len(labels), problem_type=\"multi_label_classification\")\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "# 7. Define Metrics\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions\n",
    "    probs = torch.sigmoid(torch.tensor(preds)).numpy()\n",
    "    binary_preds = (probs >= 0.5).astype(int)\n",
    "    \n",
    "    f1 = f1_score(labels, binary_preds, average='macro', zero_division=0)\n",
    "    precision = precision_score(labels, binary_preds, average='macro', zero_division=0)\n",
    "    recall = recall_score(labels, binary_preds, average='macro', zero_division=0)\n",
    "    auc = roc_auc_score(labels, probs, average='macro')\n",
    "    avg_precision = average_precision_score(labels, probs, average='macro')\n",
    "    \n",
    "    return {\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'auc': auc,\n",
    "        'average_precision': avg_precision\n",
    "    }\n",
    "\n",
    "# 8. Set Up Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',                   # Output directory\n",
    "    num_train_epochs=7,                       # Number of training epochs\n",
    "    per_device_train_batch_size=32,           # Batch size per device during training\n",
    "    per_device_eval_batch_size=32,            # Batch size for evaluation\n",
    "    learning_rate=2e-5,  # Lower learning rate for fine-tuning\n",
    "    weight_decay=0.01,                        # Strength of weight decay\n",
    "    logging_dir='./logs',                     # Directory for storing logs\n",
    "    logging_steps=10,                         # Log every 10 steps\n",
    "    evaluation_strategy=\"epoch\",              # Evaluate at the end of each epoch\n",
    "    save_strategy=\"epoch\",                    # Save checkpoint at each epoch\n",
    "    load_best_model_at_end=True,              # Load the best model when finished training\n",
    "    metric_for_best_model=\"f1\",               # Best model is determined by the F1 score\n",
    "    greater_is_better=True,                   # Whether a higher metric score is better\n",
    "    save_total_limit=2,                       # Limit the total amount of checkpoints\n",
    "    seed=42,                                   # Random seed for reproducibility\n",
    "    fp16=True,                                 # Enable mixed precision training (requires compatible hardware)\n",
    "    report_to=\"tensorboard\",\n",
    "    max_grad_norm= 1.0\n",
    "    \n",
    ")\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class WeightedBCELoss(nn.Module):\n",
    "    def __init__(self, class_weights):\n",
    "        super(WeightedBCELoss, self).__init__()\n",
    "        # Convert class_weights to a tensor; don't move to device here\n",
    "        self.class_weights = torch.tensor(class_weights).float()\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        # Ensure class_weights are on the same device as logits\n",
    "        self.class_weights = self.class_weights.to(logits.device)\n",
    "        # Compute Binary Cross-Entropy with Logits Loss without reduction\n",
    "        bce_loss = nn.BCEWithLogitsLoss(reduction='none')(logits, targets)\n",
    "        # Apply class weights\n",
    "        weighted_loss = bce_loss * self.class_weights\n",
    "        # Return the mean loss\n",
    "        return weighted_loss.mean()\n",
    "\n",
    "\n",
    "class WeightedTrainer(Trainer):\n",
    "    def __init__(self, *args, class_weights=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        if class_weights is None:\n",
    "            raise ValueError(\"class_weights must be provided\")\n",
    "        self.weighted_loss = WeightedBCELoss(class_weights).to(self.model.device)\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        # Move inputs to the same device as the model\n",
    "        inputs = {key: val.to(model.device) for key, val in inputs.items() if isinstance(val, torch.Tensor)}\n",
    "        \n",
    "        labels = inputs.pop(\"labels\")  # Extract labels\n",
    "        outputs = model(**inputs)      # Forward pass\n",
    "        logits = outputs.logits        # Get logits from model outputs\n",
    "        \n",
    "        # Compute weighted loss\n",
    "        loss = self.weighted_loss(logits, labels)\n",
    "        # Debugging: Print device info\n",
    "        # print(f\"Logits device: {logits.device}\")\n",
    "        # print(f\"Labels device: {labels.device}\")\n",
    "        # print(f\"Class Weights device: {self.weighted_loss.class_weights.device}\")\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "#8. Trainer arguments\n",
    "trainer = WeightedTrainer(\n",
    "    model=model,                             # The instantiated Transformers model to be trained\n",
    "    args=training_args,                      # Training arguments, defined above\n",
    "    train_dataset=train_dataset,             # Training dataset\n",
    "    eval_dataset=val_dataset,                # Evaluation dataset\n",
    "    compute_metrics=compute_metrics,         # Function to compute metrics\n",
    "    class_weights=class_weights_list\n",
    ")\n",
    "\n",
    "# 10. Start Training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. Evaluate Model\n",
    "results = trainer.evaluate()\n",
    "print(\"Evaluation Results:\")\n",
    "for key, value in results.items():\n",
    "    print(f\"{key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"./model\")  # Saves the model and tokenizer\n",
    "tokenizer.save_pretrained(\"./model\")  # Optional: Save tokenizer separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Assuming you are using Hugging Face's Trainer\n",
    "outputs = trainer.predict(val_dataset)  # Evaluate on the validation dataset\n",
    "\n",
    "# Extract predictions and true labels\n",
    "predictions = outputs.predictions  # Raw logits\n",
    "true_labels = outputs.label_ids     # True labels from the dataset\n",
    "\n",
    "# Convert logits to probabilities\n",
    "probabilities = torch.sigmoid(torch.tensor(predictions)).numpy()\n",
    "\n",
    "# Convert probabilities to binary predictions (threshold = 0.5 by default)\n",
    "binary_predictions = (probabilities >= 0.5).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Compute confusion matrices for each label\n",
    "confusion_matrices = {}\n",
    "for i, label in enumerate(labels):\n",
    "    cm = confusion_matrix(true_labels[:, i], binary_predictions[:, i])\n",
    "    confusion_matrices[label] = cm\n",
    "\n",
    "# Display confusion matrices\n",
    "for label, cm in confusion_matrices.items():\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=[\"Negative\", \"Positive\"], yticklabels=[\"Negative\", \"Positive\"])\n",
    "    plt.title(f\"Confusion Matrix for {label}\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.show()\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Compute metrics for each label\n",
    "for i, label in enumerate(labels):\n",
    "    precision = precision_score(true_labels[:, i], binary_predictions[:, i], zero_division=0)\n",
    "    recall = recall_score(true_labels[:, i], binary_predictions[:, i], zero_division=0)\n",
    "    f1 = f1_score(true_labels[:, i], binary_predictions[:, i], zero_division=0)\n",
    "    print(f\"Metrics for {label}: Precision={precision:.2f}, Recall={recall:.2f}, F1={f1:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
